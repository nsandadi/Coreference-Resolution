{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pooling_GAP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnUgd2j+NwQbIjS9asJES3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsandadi/Coreference-Resolution/blob/main/Pooling_GAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miGGACRfYceX"
      },
      "source": [
        "# POOLING\n",
        "### BERT, RoBERTa, CorefRoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xEVic-nYsUa"
      },
      "source": [
        "## Install Transformers library from Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT2LelAsRXpZ",
        "outputId": "32174b76-85b3-4ca8-b0f2-13ae79619174"
      },
      "source": [
        "# !pip install transformers\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-w3a4hpem\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-w3a4hpem\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 13.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 33.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2090005 sha256=8d3736ee7bac176b31b929d9634639dff9b8db301cbcba826af015354a2850a7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a220c9cv/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "Successfully built transformers\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=3d2932cbd60ba55dcd24d68f1863879080717fe12817ce65f6444136a4a76d9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.6.0.dev0\n",
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/79/64c0815cbe8c6abd7fe5525ec37a2689d3cf10e387629ba4a6e44daff6d0/boto3-1.17.49-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 22.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.49\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/59/6e28ce58206039ad2592992b75ee79a8f9dbc902a9704373ddacc4f96300/botocore-1.20.49-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 28.5MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.49->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.49->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.20.49 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.17.49 botocore-1.20.49 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC5iLTilY6O-"
      },
      "source": [
        "## Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvwm9_oYRc8z"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, BatchSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import re\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import timeit"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss5trnOvZIGt"
      },
      "source": [
        "## Load data and packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y814cuSVZHVE",
        "outputId": "8e5f5dd8-a81c-4137-aa37-2aa96dc941e0"
      },
      "source": [
        "print('installing apex')\n",
        "os.system('git clone -q https://github.com/NVIDIA/apex.git')\n",
        "os.system('pip install -q --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex/')\n",
        "os.system('rm -rf apex')\n",
        "\n",
        "\n",
        "print('downloading data')\n",
        "os.system('pip install pytorch-pretrained-bert -q')\n",
        "os.system('wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-development.tsv -q')\n",
        "os.system('wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-test.tsv -q')\n",
        "os.system('wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-validation.tsv -q')\n",
        "\n",
        "\n",
        "print('loading data')\n",
        "gap_dev = pd.read_csv('gap-development.tsv', delimiter='\\t')\n",
        "gap_val = pd.read_csv('gap-validation.tsv', delimiter='\\t')\n",
        "gap_test = pd.read_csv('gap-test.tsv', delimiter='\\t')\n",
        "\n",
        "all_data = pd.concat([gap_dev, gap_val, gap_test])\n",
        "all_data = all_data.reset_index(drop=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "installing apex\n",
            "downloading data\n",
            "loading data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnkKNYFTZWSe"
      },
      "source": [
        "## Download pre-trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH0m1cx6ZacX"
      },
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, WordpieceTokenizer\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KxPQQJJZhAZ"
      },
      "source": [
        "######## hyper-parameters tuning ######\n",
        "# BERT_NAME = 'bert-large-uncased'\n",
        "BERT_NAME = 'roberta-large'\n",
        "# BERT_NAME = \"nielsr/coref-roberta-large\"\n",
        "BERT_SIZE = 1024  # 768 for base, 1024 for large\n",
        "SEED = 23\n",
        "L = 8\n",
        "S_DIM = 16\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # If model is BERT\n",
        "# tokenizer = BertTokenizer.from_pretrained(BERT_NAME)\n",
        "# bert = BertModel.from_pretrained(BERT_NAME)\n",
        "# bert = bert.to(device)\n",
        "\n",
        "# If model is RoBERTa\n",
        "bert = RobertaModel.from_pretrained(BERT_NAME, output_hidden_states=True).cuda()\n",
        "tokenizer = RobertaTokenizer.from_pretrained(BERT_NAME)\n",
        "\n",
        "# # If model is CorefBERT or CorefRoBERTa\n",
        "# bert = AutoModel.from_pretrained(BERT_NAME, output_hidden_states= True).cuda()\n",
        "# tokenizer = AutoTokenizer.from_pretrained(BERT_NAME)\n",
        "# bert = bert.to(device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsqvb-tCbWgd"
      },
      "source": [
        "## Model Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHtPifx5RsDY",
        "outputId": "83df5563-76c3-46e7-e7c8-177e340e4c32"
      },
      "source": [
        "## Tokenize\n",
        "def bert_tokenize(text, p, a, b, p_offset, a_offset, b_offset):\n",
        "    idxs = {}\n",
        "    tokens = []\n",
        "    \n",
        "    a_span = [a_offset, a_offset+len(a), 'a']\n",
        "    b_span = [b_offset, b_offset+len(b), 'b']\n",
        "    p_span = [p_offset, p_offset+len(p), 'p']\n",
        "    \n",
        "    spans = [a_span, b_span, p_span]\n",
        "    spans = sorted(spans, key=lambda x: x[0])\n",
        "    \n",
        "    last_offset = 0\n",
        "    idx = -1\n",
        "    \n",
        "    def token_part(string):\n",
        "        _idxs = []\n",
        "        nonlocal idx\n",
        "        for w in tokenizer.tokenize(string):\n",
        "            idx += 1\n",
        "            tokens.append(w)\n",
        "            _idxs.append(idx)\n",
        "        return _idxs\n",
        "    \n",
        "    \n",
        "    for span in spans:\n",
        "        token_part(text[last_offset:span[0]])\n",
        "        idxs[span[2]] = token_part(text[span[0]:span[1]])\n",
        "        last_offset = span[1]\n",
        "    token_part(text[last_offset:])\n",
        "    return tokens, idxs\n",
        "\n",
        "print('tokenize...')\n",
        "_ = all_data.apply(lambda x: bert_tokenize(x['Text'], x['Pronoun'], x['A'], x['B'], x['Pronoun-offset'], x['A-offset'], x['B-offset']), axis=1)\n",
        "all_data['encode'] = [tokenizer.convert_tokens_to_ids(i[0]) for i in _]\n",
        "all_data['p_idx'] = [i[1]['p'] for i in _]\n",
        "all_data['a_idx'] = [i[1]['a'] for i in _]\n",
        "all_data['b_idx'] = [i[1]['b'] for i in _]\n",
        "\n",
        "## Data pre-processing\n",
        "print('clean..')\n",
        "all_data.at[2602, 'encode'] = all_data.loc[2602, 'encode'][:280]\n",
        "all_data.at[3674, 'encode'] = all_data.loc[3674, 'encode'][:280]  # too long, target in head\n",
        "all_data.at[209, 'encode'] = all_data.loc[209, 'encode'][60:]\n",
        "all_data.at[209, 'a_idx'] = [_ - 60 for _ in all_data.loc[209, 'a_idx']]  # too log, traget in tail\n",
        "all_data.at[209, 'b_idx'] = [_ - 60 for _ in all_data.loc[209, 'b_idx']]\n",
        "all_data.at[209, 'p_idx'] = [_ - 60 for _ in all_data.loc[209, 'p_idx']]\n",
        "\n",
        "\n",
        "class GPTData(Dataset):\n",
        "    \n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        _ = self.data.loc[idx]\n",
        "        sample = {'id': _['ID'],\n",
        "                  'encode': torch.LongTensor([101] + _['encode'] + [102]),\n",
        "                  'p_idx': torch.LongTensor(_['p_idx'])+1,\n",
        "                  'a_idx': torch.LongTensor(_['a_idx'])+1,\n",
        "                  'b_idx': torch.LongTensor(_['b_idx'])+1,\n",
        "                  'coref': torch.LongTensor([0 if _['A-coref'] else 1 if _['B-coref'] else 2])\n",
        "                 }\n",
        "        return sample\n",
        "        \n",
        "class SortLenSampler(Sampler):\n",
        "    \n",
        "    def __init__(self, data_source, key):\n",
        "        self.sorted_idx = sorted(range(len(data_source)), key=lambda x: len(data_source[x][key]))\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return iter(self.sorted_idx)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sorted_idx)\n",
        "        \n",
        "\n",
        "def gpt_collate_func(x):\n",
        "    _ = [[], [], [], [], [], []]\n",
        "    for i in x:\n",
        "        _[0].append(i['encode'])\n",
        "        _[1].append(i['p_idx'])\n",
        "        _[2].append(i['a_idx'])\n",
        "        _[3].append(i['b_idx'])\n",
        "        _[4].append(i['coref'])\n",
        "        _[5].append(i['id'])\n",
        "    return torch.nn.utils.rnn.pad_sequence(_[0], batch_first=True, padding_value=0), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(_[1], batch_first=True, padding_value=-1), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(_[2], batch_first=True, padding_value=-1), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(_[3], batch_first=True, padding_value=-1), \\\n",
        "           torch.cat(_[4], dim=0), _[5]\n",
        "\n",
        "## Pooling\n",
        "def meanpooling(x, idx, pad=-1):\n",
        "    \"\"\"x: Layer X Seq X Feat, idx: Seq \"\"\"\n",
        "    t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "    _ = torch.zeros((x.shape[0], x.shape[2]))\n",
        "    cnt = 0\n",
        "    for i in idx:\n",
        "        if i == pad:\n",
        "            break\n",
        "        for j in range(x.shape[0]):\n",
        "            _[j] += x[j,i,:]\n",
        "        cnt += 1\n",
        "    \n",
        "    if cnt == 0:\n",
        "        raise ValueError('0 dive')\n",
        "    mean = _/cnt\n",
        "    return mean\n",
        "\n",
        "def sumpooling(x, idx, pad=-1):\n",
        "    \"\"\"x: Layer X Seq X Feat, idx: Seq \"\"\"\n",
        "    t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "    _ = torch.zeros((x.shape[0], x.shape[2]))\n",
        "    cnt = 0\n",
        "    for i in idx:\n",
        "        if i == pad:\n",
        "            break\n",
        "        for j in range(x.shape[0]):\n",
        "            _[j] += x[j,i,:]\n",
        "    return _\n",
        "\n",
        "def maxpooling(x, idx, pad=-1):\n",
        "  \"\"\"x: Layer X Seq X Feat, idx: Seq\"\"\"\n",
        "  t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "  _ = torch.full((x.shape[0], x.shape[2]), -float('inf'))\n",
        "  for i in idx:\n",
        "      if i == pad:\n",
        "          break\n",
        "      for j in range(x.shape[0]):\n",
        "          for k in range(x.shape[2]):\n",
        "              _[j][k] = torch.max(_[j][k], x[j,i,:][k])\n",
        "  return _\n",
        "\n",
        "\n",
        "def minpooling(x, idx, pad=-1):\n",
        "  \"\"\"x: Layer X Seq X Feat, idx: Seq\"\"\"\n",
        "  t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "  _ = torch.full((x.shape[0], x.shape[2]), float('inf'))\n",
        "  for i in idx:\n",
        "      if i == pad:\n",
        "          break\n",
        "      for j in range(x.shape[0]):\n",
        "          for k in range(x.shape[2]):\n",
        "              _[j][k] = torch.min(_[j][k], x[j,i,:][k])\n",
        "  return _\n",
        "\n",
        "\n",
        "def get_span_tensor(bert_t, index, last_layer=L, pad_id=-1):\n",
        "    \"\"\"return Seq X Layer X Feat\"\"\"\n",
        "    span_tensor = []\n",
        "    for i in index:\n",
        "        if i == pad_id:\n",
        "            break\n",
        "        # span_tensor.append(bert_t[16:21, i, :])\n",
        "        span_tensor.append(bert_t[-last_layer:, i, :])\n",
        "    return torch.stack(span_tensor)\n",
        "    \n",
        "\n",
        "_ = GPTData(all_data)\n",
        "gpt_iter = DataLoader(_, batch_size=5, sampler=SortLenSampler(_, 'encode'), collate_fn=gpt_collate_func)\n",
        "\n",
        "## Extract BERT features\n",
        "bert_feats = []\n",
        "print('extract bert features..')\n",
        "start = timeit.default_timer()\n",
        "bert.eval()\n",
        "for (x, p, a, b, y, id_) in gpt_iter:\n",
        "    r = bert.forward(x.cuda(), attention_mask= (x!=0).cuda())\n",
        "    # _ = torch.stack(r[0][-L:]).cpu().data.clone()  ## For BERT\n",
        "    _ = torch.stack(r[2][-L:]).cpu().data.clone()  ## For RoBERTa\n",
        "    del(r)\n",
        "    for i, v in enumerate(id_):\n",
        "        bert_feats.append({'a': meanpooling(_[:,i,:],a[i]),\n",
        "                           'b': meanpooling(_[:,i,:],b[i]),\n",
        "                           'p': meanpooling(_[:,i,:],p[i]),\n",
        "                           'ap': (a[i][0] - p[i][0]).type(torch.FloatTensor),\n",
        "                           'bp': (b[i][0] - p[i][0]).type(torch.FloatTensor),\n",
        "                           'y': y[i],\n",
        "                           'id': v})\n",
        "\n",
        "print('extract bert features finished.')\n",
        "stop = timeit.default_timer()\n",
        "print('Runtime: ', stop - start)        \n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "############\n",
        "\n",
        "class BERTfeature(Dataset):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "    \n",
        "def bert_collate_func(x):\n",
        "    _ = [[] for i in range(6)]\n",
        "    for i in x:\n",
        "        _[0].append(i['a'])\n",
        "        _[1].append(i['b'])\n",
        "        _[2].append(i['p'])\n",
        "        _[3].append(i['y'])\n",
        "        _[4].append(i['ap'])\n",
        "        _[5].append(i['bp'])\n",
        "    return [torch.stack(i) for i in _]\n",
        "\n",
        "\n",
        "## Split into train & test\n",
        "test = [i for i in bert_feats if 'dev' in i['id']]\n",
        "train = [i for i in bert_feats if 'dev' not in i['id']]\n",
        "\n",
        "############\n",
        "\n",
        "class SimilarityLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_dim, dropout=0.3):\n",
        "        super(SimilarityLayer, self).__init__()\n",
        "        self.ffnn = nn.Linear(hidden_dim*5, S_DIM)\n",
        "        nn.init.kaiming_normal_(self.ffnn.weight)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, a, b, p):\n",
        "        _input = torch.cat([p, a, b, p*a, p*b], dim=1)\n",
        "        y = self.ffnn(self.dropout(_input))\n",
        "        \n",
        "        return y\n",
        "    \n",
        "\n",
        "class MSnet(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_dim, dropout=0.5, hidden_layer=4):\n",
        "        super(MSnet, self).__init__()\n",
        "        self.sim_layers = nn.ModuleList([SimilarityLayer(hidden_dim, dropout=dropout) for i in range(hidden_layer)])\n",
        "        self.bn = nn.BatchNorm1d(S_DIM*hidden_layer)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.mention_score = nn.Linear(S_DIM*hidden_layer+2, 3)\n",
        "        self.dist_ecoding = nn.Linear(1,1)\n",
        "        \n",
        "    def forward(self, a, b, p, ap, bp):\n",
        "        y = []\n",
        "        for i, l in enumerate(self.sim_layers):\n",
        "            y.append(l(a[:,i,:], b[:,i,:], p[:,i,:]))\n",
        "        y = torch.cat(y, dim=1) # B X 64*Layer\n",
        "        y = self.dropout(self.bn(y).relu())\n",
        "        ap = self.dist_ecoding(ap[:,None]).tanh()\n",
        "        bp = self.dist_ecoding(bp[:,None]).tanh()\n",
        "        return self.mention_score(torch.cat([y, ap, bp], dim=1))\n",
        "\n",
        "\n",
        "def training_cuda(epoch, model, lossfunc, optimizer, train_iter, val_iter, test_iter, start=5):\n",
        "    best_score = 10\n",
        "    for i in range(epoch):\n",
        "        model.train()\n",
        "        epoch_score = np.array([])\n",
        "        for (a, b, p, y, ap, bp) in iter(train_iter):\n",
        "            model.zero_grad()\n",
        "            pred = model.forward(a.cuda(), b.cuda(), p.cuda(), ap.cuda(), bp.cuda())\n",
        "            # loss = lossfunc(pred, y.cuda()) + l2 * torch.stack([torch.norm(i[1]) for i in model.named_parameters() if 'weight' in i[0]]).sum()\n",
        "            loss = lossfunc(pred, y.cuda())\n",
        "            s = score(pred.softmax(1), y.cuda())\n",
        "            epoch_score = np.append(epoch_score, s.cpu().data.numpy())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            model.zero_grad()\n",
        "            val_score =  np.array([])\n",
        "            for (va, vb, vp, vy, vap, vbp) in val_iter:\n",
        "                vpred = model.forward(va.cuda(), vb.cuda(), vp.cuda(), vap.cuda(), vbp.cuda())\n",
        "                vs = score(vpred.softmax(1), vy.cuda())\n",
        "                val_score = np.append(val_score, vs.cpu().data.numpy())\n",
        "            print('epcoh {:02} - train_score {:.4f} - val_score {:.4f} '.format(\n",
        "                                i, np.mean(epoch_score), np.mean(val_score)))\n",
        "            if  np.mean(val_score) < best_score:\n",
        "                best_score = np.mean(val_score)\n",
        "                if i > start:\n",
        "                    torch.save(model.state_dict(), 'tmp.m')\n",
        "    model.load_state_dict(torch.load('tmp.m'))\n",
        "    test_pred = np.array([])\n",
        "    for (ta, tb, tp, ty, tap, tbp) in test_iter:\n",
        "        vpred = model.forward(ta.cuda(), tb.cuda(), tp.cuda(), tap.cuda(), tbp.cuda())\n",
        "        test_pred = np.append(test_pred, vpred.softmax(1).cpu().data.numpy())\n",
        "    return best_score, test_pred\n",
        "\n",
        "\n",
        "def score(pred, y):\n",
        "    t_float = torch.FloatTensor\n",
        "    if isinstance(pred, torch.cuda.FloatTensor):\n",
        "        t_float = torch.cuda.FloatTensor\n",
        "    y = (torch.cumsum(torch.ones(y.shape[0], 3), dim=1) -1).type(t_float) == y[:,None].type(t_float)\n",
        "    s = (y.type(t_float) * pred).sum(1).log()\n",
        "    return -s\n",
        "\n",
        "## Training\n",
        "print('training')\n",
        "m = MSnet(BERT_SIZE, dropout=0.4, hidden_layer=L).cuda()\n",
        "optimizer = optim.Adam(m.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "loss_fuc = nn.CrossEntropyLoss()\n",
        "batch_size = 32\n",
        "\n",
        "kfold = KFold(n_splits=5, random_state=SEED, shuffle=True)\n",
        "scores = []\n",
        "m_s = deepcopy(m.state_dict().copy())\n",
        "opt_s = deepcopy(optimizer.state_dict().copy())\n",
        "\n",
        "k_th = 0\n",
        "test_iter = DataLoader(BERTfeature(test), batch_size=batch_size, shuffle=False, collate_fn=bert_collate_func)\n",
        "test_preds = []\n",
        "\n",
        "for train_idx, val_idx in kfold.split(list(range(len(train)))):\n",
        "    \n",
        "    _train = [v for i, v in enumerate(train) if i in train_idx]\n",
        "    _val = [v for i, v in enumerate(train) if i in val_idx]\n",
        "    train_iter = DataLoader(BERTfeature(_train), batch_size=batch_size, shuffle=True, collate_fn=bert_collate_func)\n",
        "    val_iter = DataLoader(BERTfeature(_val), batch_size=batch_size, shuffle=False, collate_fn=bert_collate_func)\n",
        "    \n",
        "    m.load_state_dict(m_s)\n",
        "    optimizer.load_state_dict(opt_s)\n",
        "    s, y = training_cuda(30, m, loss_fuc, optimizer, train_iter, val_iter, test_iter)\n",
        "    scores.append(s)\n",
        "    test_preds.append(y)\n",
        "    \n",
        "    k_th += 1\n",
        "    print('------------'*3)\n",
        "    \n",
        "print('Score: {:.4f} {:.4f}'.format(np.mean(scores), np.std(scores)))\n",
        "probs = np.mean(test_preds, axis=0).reshape((-1, 3))\n",
        "true = torch.cat([ty for (ta, tb, tp, ty, tap, tbp) in test_iter], dim=0).data.numpy()\n",
        "t_ids = [i['id'] for i in test]\n",
        "print(log_loss(true, probs))\n",
        "\n",
        "## Accuracy and F1-score\n",
        "acc_pred = []\n",
        "for i in range(len(probs)):\n",
        "    acc_pred.append(list(probs[i]).index(max(probs[i])))\n",
        "acc_pred = np.asarray(acc_pred)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy:\",accuracy_score(true, acc_pred))\n",
        "print(\"F1 score:\", f1_score(true, acc_pred, average=None))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenize...\n",
            "clean..\n",
            "extract bert features..\n",
            "extract bert features finished.\n",
            "Runtime:  43.43034713999987\n",
            "training\n",
            "epcoh 00 - train_score 1.0570 - val_score 0.9331 \n",
            "epcoh 01 - train_score 0.8668 - val_score 0.7827 \n",
            "epcoh 02 - train_score 0.7168 - val_score 0.6807 \n",
            "epcoh 03 - train_score 0.6226 - val_score 0.5869 \n",
            "epcoh 04 - train_score 0.5416 - val_score 0.5381 \n",
            "epcoh 05 - train_score 0.4802 - val_score 0.4776 \n",
            "epcoh 06 - train_score 0.4056 - val_score 0.4315 \n",
            "epcoh 07 - train_score 0.3701 - val_score 0.4107 \n",
            "epcoh 08 - train_score 0.3132 - val_score 0.3741 \n",
            "epcoh 09 - train_score 0.2939 - val_score 0.3595 \n",
            "epcoh 10 - train_score 0.2573 - val_score 0.3641 \n",
            "epcoh 11 - train_score 0.2246 - val_score 0.3509 \n",
            "epcoh 12 - train_score 0.2016 - val_score 0.3507 \n",
            "epcoh 13 - train_score 0.1895 - val_score 0.3422 \n",
            "epcoh 14 - train_score 0.1626 - val_score 0.3360 \n",
            "epcoh 15 - train_score 0.1544 - val_score 0.3390 \n",
            "epcoh 16 - train_score 0.1426 - val_score 0.3560 \n",
            "epcoh 17 - train_score 0.1218 - val_score 0.3411 \n",
            "epcoh 18 - train_score 0.1116 - val_score 0.3656 \n",
            "epcoh 19 - train_score 0.1074 - val_score 0.3472 \n",
            "epcoh 20 - train_score 0.0837 - val_score 0.3643 \n",
            "epcoh 21 - train_score 0.0745 - val_score 0.3651 \n",
            "epcoh 22 - train_score 0.0814 - val_score 0.3571 \n",
            "epcoh 23 - train_score 0.0705 - val_score 0.3471 \n",
            "epcoh 24 - train_score 0.0592 - val_score 0.3629 \n",
            "epcoh 25 - train_score 0.0593 - val_score 0.3674 \n",
            "epcoh 26 - train_score 0.0633 - val_score 0.3976 \n",
            "epcoh 27 - train_score 0.0591 - val_score 0.3883 \n",
            "epcoh 28 - train_score 0.0457 - val_score 0.3818 \n",
            "epcoh 29 - train_score 0.0435 - val_score 0.3944 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 1.0590 - val_score 0.9323 \n",
            "epcoh 01 - train_score 0.8590 - val_score 0.7916 \n",
            "epcoh 02 - train_score 0.7180 - val_score 0.6467 \n",
            "epcoh 03 - train_score 0.6081 - val_score 0.5759 \n",
            "epcoh 04 - train_score 0.5289 - val_score 0.5242 \n",
            "epcoh 05 - train_score 0.4529 - val_score 0.4809 \n",
            "epcoh 06 - train_score 0.3793 - val_score 0.4584 \n",
            "epcoh 07 - train_score 0.3587 - val_score 0.4226 \n",
            "epcoh 08 - train_score 0.3017 - val_score 0.4182 \n",
            "epcoh 09 - train_score 0.2685 - val_score 0.4000 \n",
            "epcoh 10 - train_score 0.2420 - val_score 0.3991 \n",
            "epcoh 11 - train_score 0.2038 - val_score 0.3964 \n",
            "epcoh 12 - train_score 0.1898 - val_score 0.3978 \n",
            "epcoh 13 - train_score 0.1704 - val_score 0.3905 \n",
            "epcoh 14 - train_score 0.1505 - val_score 0.3954 \n",
            "epcoh 15 - train_score 0.1508 - val_score 0.4078 \n",
            "epcoh 16 - train_score 0.1276 - val_score 0.4017 \n",
            "epcoh 17 - train_score 0.1085 - val_score 0.4000 \n",
            "epcoh 18 - train_score 0.1016 - val_score 0.4143 \n",
            "epcoh 19 - train_score 0.0902 - val_score 0.4235 \n",
            "epcoh 20 - train_score 0.0805 - val_score 0.4339 \n",
            "epcoh 21 - train_score 0.0737 - val_score 0.4304 \n",
            "epcoh 22 - train_score 0.0742 - val_score 0.4355 \n",
            "epcoh 23 - train_score 0.0683 - val_score 0.4435 \n",
            "epcoh 24 - train_score 0.0661 - val_score 0.4376 \n",
            "epcoh 25 - train_score 0.0532 - val_score 0.4601 \n",
            "epcoh 26 - train_score 0.0473 - val_score 0.4605 \n",
            "epcoh 27 - train_score 0.0418 - val_score 0.4617 \n",
            "epcoh 28 - train_score 0.0482 - val_score 0.4697 \n",
            "epcoh 29 - train_score 0.0532 - val_score 0.4523 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 1.0651 - val_score 0.9622 \n",
            "epcoh 01 - train_score 0.8402 - val_score 0.8200 \n",
            "epcoh 02 - train_score 0.6993 - val_score 0.7050 \n",
            "epcoh 03 - train_score 0.5978 - val_score 0.6268 \n",
            "epcoh 04 - train_score 0.5192 - val_score 0.5617 \n",
            "epcoh 05 - train_score 0.4480 - val_score 0.5247 \n",
            "epcoh 06 - train_score 0.3965 - val_score 0.4835 \n",
            "epcoh 07 - train_score 0.3569 - val_score 0.4796 \n",
            "epcoh 08 - train_score 0.3083 - val_score 0.4506 \n",
            "epcoh 09 - train_score 0.2714 - val_score 0.4408 \n",
            "epcoh 10 - train_score 0.2447 - val_score 0.4325 \n",
            "epcoh 11 - train_score 0.2326 - val_score 0.4360 \n",
            "epcoh 12 - train_score 0.1927 - val_score 0.4150 \n",
            "epcoh 13 - train_score 0.1793 - val_score 0.4050 \n",
            "epcoh 14 - train_score 0.1618 - val_score 0.4122 \n",
            "epcoh 15 - train_score 0.1477 - val_score 0.4099 \n",
            "epcoh 16 - train_score 0.1310 - val_score 0.4040 \n",
            "epcoh 17 - train_score 0.1238 - val_score 0.4138 \n",
            "epcoh 18 - train_score 0.0986 - val_score 0.4152 \n",
            "epcoh 19 - train_score 0.0960 - val_score 0.4086 \n",
            "epcoh 20 - train_score 0.0928 - val_score 0.4119 \n",
            "epcoh 21 - train_score 0.0802 - val_score 0.4239 \n",
            "epcoh 22 - train_score 0.0768 - val_score 0.4200 \n",
            "epcoh 23 - train_score 0.0768 - val_score 0.4259 \n",
            "epcoh 24 - train_score 0.0614 - val_score 0.4454 \n",
            "epcoh 25 - train_score 0.0745 - val_score 0.4270 \n",
            "epcoh 26 - train_score 0.0641 - val_score 0.4452 \n",
            "epcoh 27 - train_score 0.0529 - val_score 0.4390 \n",
            "epcoh 28 - train_score 0.0579 - val_score 0.4550 \n",
            "epcoh 29 - train_score 0.0414 - val_score 0.4588 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 1.0552 - val_score 0.9453 \n",
            "epcoh 01 - train_score 0.8361 - val_score 0.8145 \n",
            "epcoh 02 - train_score 0.7119 - val_score 0.6593 \n",
            "epcoh 03 - train_score 0.6085 - val_score 0.5950 \n",
            "epcoh 04 - train_score 0.5320 - val_score 0.5414 \n",
            "epcoh 05 - train_score 0.4561 - val_score 0.4880 \n",
            "epcoh 06 - train_score 0.4101 - val_score 0.4671 \n",
            "epcoh 07 - train_score 0.3450 - val_score 0.4346 \n",
            "epcoh 08 - train_score 0.3128 - val_score 0.4302 \n",
            "epcoh 09 - train_score 0.2846 - val_score 0.4143 \n",
            "epcoh 10 - train_score 0.2484 - val_score 0.4164 \n",
            "epcoh 11 - train_score 0.2249 - val_score 0.4083 \n",
            "epcoh 12 - train_score 0.2032 - val_score 0.3946 \n",
            "epcoh 13 - train_score 0.1777 - val_score 0.3950 \n",
            "epcoh 14 - train_score 0.1454 - val_score 0.4024 \n",
            "epcoh 15 - train_score 0.1405 - val_score 0.4126 \n",
            "epcoh 16 - train_score 0.1210 - val_score 0.4176 \n",
            "epcoh 17 - train_score 0.1175 - val_score 0.4194 \n",
            "epcoh 18 - train_score 0.0982 - val_score 0.4276 \n",
            "epcoh 19 - train_score 0.0951 - val_score 0.4493 \n",
            "epcoh 20 - train_score 0.0952 - val_score 0.4548 \n",
            "epcoh 21 - train_score 0.0878 - val_score 0.4558 \n",
            "epcoh 22 - train_score 0.0725 - val_score 0.4415 \n",
            "epcoh 23 - train_score 0.0548 - val_score 0.4582 \n",
            "epcoh 24 - train_score 0.0527 - val_score 0.4578 \n",
            "epcoh 25 - train_score 0.0522 - val_score 0.4533 \n",
            "epcoh 26 - train_score 0.0495 - val_score 0.4538 \n",
            "epcoh 27 - train_score 0.0419 - val_score 0.4733 \n",
            "epcoh 28 - train_score 0.0477 - val_score 0.4835 \n",
            "epcoh 29 - train_score 0.0512 - val_score 0.4714 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 1.0613 - val_score 0.9340 \n",
            "epcoh 01 - train_score 0.8478 - val_score 0.8012 \n",
            "epcoh 02 - train_score 0.7158 - val_score 0.6405 \n",
            "epcoh 03 - train_score 0.6145 - val_score 0.5723 \n",
            "epcoh 04 - train_score 0.5312 - val_score 0.5210 \n",
            "epcoh 05 - train_score 0.4629 - val_score 0.4852 \n",
            "epcoh 06 - train_score 0.4044 - val_score 0.4558 \n",
            "epcoh 07 - train_score 0.3521 - val_score 0.4398 \n",
            "epcoh 08 - train_score 0.3046 - val_score 0.4169 \n",
            "epcoh 09 - train_score 0.2695 - val_score 0.4510 \n",
            "epcoh 10 - train_score 0.2455 - val_score 0.4066 \n",
            "epcoh 11 - train_score 0.2100 - val_score 0.3883 \n",
            "epcoh 12 - train_score 0.1932 - val_score 0.4137 \n",
            "epcoh 13 - train_score 0.1624 - val_score 0.4036 \n",
            "epcoh 14 - train_score 0.1515 - val_score 0.4340 \n",
            "epcoh 15 - train_score 0.1470 - val_score 0.4304 \n",
            "epcoh 16 - train_score 0.1310 - val_score 0.4135 \n",
            "epcoh 17 - train_score 0.1157 - val_score 0.4138 \n",
            "epcoh 18 - train_score 0.1088 - val_score 0.4347 \n",
            "epcoh 19 - train_score 0.0955 - val_score 0.4376 \n",
            "epcoh 20 - train_score 0.0956 - val_score 0.4346 \n",
            "epcoh 21 - train_score 0.0774 - val_score 0.4419 \n",
            "epcoh 22 - train_score 0.0810 - val_score 0.4675 \n",
            "epcoh 23 - train_score 0.0723 - val_score 0.4209 \n",
            "epcoh 24 - train_score 0.0584 - val_score 0.4297 \n",
            "epcoh 25 - train_score 0.0546 - val_score 0.4271 \n",
            "epcoh 26 - train_score 0.0524 - val_score 0.4546 \n",
            "epcoh 27 - train_score 0.0461 - val_score 0.4428 \n",
            "epcoh 28 - train_score 0.0532 - val_score 0.4429 \n",
            "epcoh 29 - train_score 0.0526 - val_score 0.4443 \n",
            "------------------------------------\n",
            "Score: 0.3827 0.0239\n",
            "0.3561833977641095\n",
            "Accuracy: 0.88\n",
            "F1 score: [0.90227273 0.8953108  0.71428571]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}