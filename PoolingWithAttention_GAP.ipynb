{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PoolingWithAttention_GAP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZGpUZYlcoFGyZ50YmsVXv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsandadi/Coreference-Resolution/blob/main/PoolingWithAttention_GAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e60rZRQeiRSW"
      },
      "source": [
        "## Meanpooling with Attention (GAP)\n",
        "### BERT, RoBERTa, CorefRoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyjgW5mwYvYd"
      },
      "source": [
        "## Install Transformers library from Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5g_dfssX4nz",
        "outputId": "34167a74-1834-481f-dc10-638d9f250f50"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-0kw305xr\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-0kw305xr\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 16.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2090005 sha256=5c46a1b989afb50db052c7bb0335527302ff50619a4560aabc74ae49e7c930f9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wq0vnqmt/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "Successfully built transformers\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=d858ec0cf40dbe76c4e4b8699d3a2aa06c0f066df60fb1450c1a48ae686760c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.6.0.dev0\n",
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 16.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/1e/570e2446e97bac3d348d0bc6cbf8ac28997ddbef3d97c052f1c476ff48bb/boto3-1.17.49.tar.gz (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Collecting botocore<1.21.0,>=1.20.49\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/59/6e28ce58206039ad2592992b75ee79a8f9dbc902a9704373ddacc4f96300/botocore-1.20.49-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 18.7MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.49->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.49->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Building wheels for collected packages: boto3\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.17.49-py2.py3-none-any.whl size=128780 sha256=28ef2ba065d88fd67a6e011f3d8abd6ebca6ab1e707c17fe747f25c2acbb0001\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/b1/01/9e2fc2b05c6a254c29192a33e40dec0486588261372745ab27\n",
            "Successfully built boto3\n",
            "\u001b[31mERROR: botocore 1.20.49 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.17.49 botocore-1.20.49 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh7W8gCGY3zp"
      },
      "source": [
        "## Import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KejxTfJVX-sY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, BatchSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import re\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import timeit"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "652cHGURZBQn"
      },
      "source": [
        "## Download data and packages (for gpu)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLxi5M7OYSY5",
        "outputId": "77b0d179-2988-471d-d522-2d1ebcf517df"
      },
      "source": [
        "print('installing apex')\n",
        "os.system('git clone -q https://github.com/NVIDIA/apex.git')\n",
        "os.system('pip install -q --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex/')\n",
        "os.system('rm -rf apex')\n",
        "\n",
        "print('download data')\n",
        "os.system('pip install pytorch-pretrained-bert -q')\n",
        "os.system('wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-development.tsv -q')\n",
        "os.system('wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-test.tsv -q')\n",
        "os.system('wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-validation.tsv -q')\n",
        "\n",
        "\n",
        "print('load data')\n",
        "gap_dev = pd.read_csv('gap-development.tsv', delimiter='\\t')\n",
        "gap_val = pd.read_csv('gap-validation.tsv', delimiter='\\t')\n",
        "gap_test = pd.read_csv('gap-test.tsv', delimiter='\\t')\n",
        "\n",
        "all_data = pd.concat([gap_dev, gap_val, gap_test])\n",
        "all_data = all_data.reset_index(drop=True)\n",
        "\n",
        "# # For training on balanced and testing on gender unbalanced dataset\n",
        "# gap_dev_male = gap_dev[gap_test.Pronoun.isin(['he', 'him', 'his','He','Him','His'])]\n",
        "# gap_dev_female = gap_dev[gap_test.Pronoun.isin(['she', 'her', 'hers','She','Her','Hers'])]\n",
        "# all_data = pd.concat([gap_dev_male, gap_val, gap_test])\n",
        "# all_data = all_data.reset_index(drop=True)\n",
        "\n",
        "# # For training on gender unbalanced dataset and testing on balanced dataset\n",
        "# gap_test_male = gap_test[gap_test.Pronoun.isin(['he', 'him', 'his','He','Him','His'])]\n",
        "# gap_val_male  = gap_val[gap_test.Pronoun.isin(['he', 'him', 'his','He','Him','His'])]\n",
        "# gap_test_female = gap_test[gap_test.Pronoun.isin(['she', 'her', 'hers','She','Her','Hers'])]\n",
        "# gap_val_female = gap_val[gap_test.Pronoun.isin(['she', 'her', 'hers','She','Her','Hers'])]\n",
        "# all_data = pd.concat([gap_dev, gap_val_male, gap_test_male])\n",
        "# all_data = all_data.reset_index(drop=True)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "installing apex\n",
            "download data\n",
            "load data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5ulxOK5ZKTb"
      },
      "source": [
        "## Download BERT, RoBERTa, CorefRoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAaeXB1VZZBa"
      },
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, WordpieceTokenizer\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K80DduNcZ97x"
      },
      "source": [
        "### Model selection & hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhoZvlWfYTYL"
      },
      "source": [
        "# BERT_NAME = 'bert-large-uncased'\n",
        "# BERT_NAME = 'roberta-large'\n",
        "ERT_NAME = \"nielsr/coref-roberta-large\"\n",
        "BERT_SIZE = 1024  \n",
        "SEED = 23\n",
        "L = 5\n",
        "S_DIM = 32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # If model is BERT \n",
        "# tokenizer = BertTokenizer.from_pretrained(BERT_NAME)\n",
        "# bert = BertModel.from_pretrained(BERT_NAME)\n",
        "# bert = bert.to(device)\n",
        "\n",
        "# # If model is RoBERTa\n",
        "# bert = RobertaModel.from_pretrained(BERT_NAME, output_hidden_states=True).cuda()\n",
        "# tokenizer = RobertaTokenizer.from_pretrained(BERT_NAME)\n",
        "\n",
        "# If model is CorefBERT or CorefRoBERTa\n",
        "bert = AutoModel.from_pretrained(ERT_NAME, output_hidden_states= True).cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(BERT_NAME)\n",
        "bert = bert.to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb6F6g-ijyDi"
      },
      "source": [
        "## Model Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjoMXaWEVFdN",
        "outputId": "ba067d00-a2f7-4b2d-ab78-01b6f44e96f3"
      },
      "source": [
        "## Tokenize\n",
        "def bert_tokenize(text, p, a, b, p_offset, a_offset, b_offset):\n",
        "    idxs = {}\n",
        "    tokens = []\n",
        "    \n",
        "    a_span = [a_offset, a_offset+len(a), 'a']\n",
        "    b_span = [b_offset, b_offset+len(b), 'b']\n",
        "    p_span = [p_offset, p_offset+len(p), 'p']\n",
        "    \n",
        "    spans = [a_span, b_span, p_span]\n",
        "    spans = sorted(spans, key=lambda x: x[0])\n",
        "    \n",
        "    last_offset = 0\n",
        "    idx = -1\n",
        "    \n",
        "    def token_part(string):\n",
        "        _idxs = []\n",
        "        nonlocal idx\n",
        "        for w in tokenizer.tokenize(string):\n",
        "            idx += 1\n",
        "            tokens.append(w)\n",
        "            _idxs.append(idx)\n",
        "        return _idxs\n",
        "    \n",
        "    \n",
        "    for span in spans:\n",
        "        token_part(text[last_offset:span[0]])\n",
        "        idxs[span[2]] = token_part(text[span[0]:span[1]])\n",
        "        last_offset = span[1]\n",
        "    token_part(text[last_offset:])\n",
        "    return tokens, idxs\n",
        "    \n",
        "\n",
        "print('tokenize...')\n",
        "_ = all_data.apply(lambda x: bert_tokenize(x['Text'], x['Pronoun'], x['A'], x['B'], x['Pronoun-offset'], x['A-offset'], x['B-offset']), axis=1)\n",
        "all_data['encode'] = [tokenizer.convert_tokens_to_ids(i[0]) for i in _]\n",
        "all_data['p_idx'] = [i[1]['p'] for i in _]\n",
        "all_data['a_idx'] = [i[1]['a'] for i in _]\n",
        "all_data['b_idx'] = [i[1]['b'] for i in _]\n",
        "\n",
        "## Data pre-processing\n",
        "print('clean..')\n",
        "all_data.at[2602, 'encode'] = all_data.loc[2602, 'encode'][:280]\n",
        "# all_data.at[3674, 'encode'] = all_data.loc[3674, 'encode'][:280]  # too long, target in head\n",
        "all_data.at[209, 'encode'] = all_data.loc[209, 'encode'][60:]\n",
        "all_data.at[209, 'a_idx'] = [_ - 60 for _ in all_data.loc[209, 'a_idx']]  # too log, traget in tail\n",
        "all_data.at[209, 'b_idx'] = [_ - 60 for _ in all_data.loc[209, 'b_idx']]\n",
        "all_data.at[209, 'p_idx'] = [_ - 60 for _ in all_data.loc[209, 'p_idx']]\n",
        "\n",
        "class GPTData(Dataset):\n",
        "    \n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        _ = self.data.loc[idx]\n",
        "        sample = {'id': _['ID'],\n",
        "                  'encode': torch.LongTensor([101] + _['encode'] + [102]),\n",
        "                  'p_idx': torch.LongTensor(_['p_idx'])+1,\n",
        "                  'a_idx': torch.LongTensor(_['a_idx'])+1,\n",
        "                  'b_idx': torch.LongTensor(_['b_idx'])+1,\n",
        "                  'coref': torch.LongTensor([0 if _['A-coref'] else 1 if _['B-coref'] else 2])\n",
        "                 }\n",
        "        return sample\n",
        "        \n",
        "class SortLenSampler(Sampler):\n",
        "    \n",
        "    def __init__(self, data_source, key):\n",
        "        self.sorted_idx = sorted(range(len(data_source)), key=lambda x: len(data_source[x][key]))\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return iter(self.sorted_idx)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sorted_idx)\n",
        "        \n",
        "\n",
        "def gpt_collate_func(x):\n",
        "    _ = [[], [], [], [], [], []]\n",
        "    for i in x:\n",
        "        _[0].append(i['encode'])\n",
        "        _[1].append(i['p_idx'])\n",
        "        _[2].append(i['a_idx'])\n",
        "        _[3].append(i['b_idx'])\n",
        "        _[4].append(i['coref'])\n",
        "        _[5].append(i['id'])\n",
        "    return torch.nn.utils.rnn.pad_sequence(_[0], batch_first=True, padding_value=0), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(_[1], batch_first=True, padding_value=-1), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(_[2], batch_first=True, padding_value=-1), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(_[3], batch_first=True, padding_value=-1), \\\n",
        "           torch.cat(_[4], dim=0), _[5]\n",
        "\n",
        "## Pooling\n",
        "def meanpooling(x, idx, pad=-1):\n",
        "    \"\"\"x: Layer X Seq X Feat, idx: Seq \"\"\"\n",
        "    t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "    \n",
        "    _ = torch.zeros((x.shape[0], x.shape[2]))\n",
        "    cnt = 0\n",
        "    for i in idx:\n",
        "        if i == pad:\n",
        "            break\n",
        "        for j in range(x.shape[0]):\n",
        "            _[j] += x[j,i,:]\n",
        "        cnt += 1\n",
        "    if cnt == 0:\n",
        "        raise ValueError('0 dive')\n",
        "    return _/cnt\n",
        "\n",
        "def maxpooling(x, idx, pad=-1):\n",
        "  \"\"\"x: Layer X Seq X Feat, idx: Seq\"\"\"\n",
        "  t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "  _ = torch.full((x.shape[0], x.shape[2]), -float('inf'))\n",
        "  for i in idx:\n",
        "      if i == pad:\n",
        "          break\n",
        "      for j in range(x.shape[0]):\n",
        "          for k in range(x.shape[2]):\n",
        "              _[j][k] = torch.max(_[j][k], x[j,i,:][k])\n",
        "  \n",
        "  return _\n",
        "\n",
        "\n",
        "def minpooling(x, idx, pad=-1):\n",
        "  \"\"\"x: Layer X Seq X Feat, idx: Seq\"\"\"\n",
        "  t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "  _ = torch.full((x.shape[0], x.shape[2]), float('inf'))\n",
        "  for i in idx:\n",
        "      if i == pad:\n",
        "          break\n",
        "      for j in range(x.shape[0]):\n",
        "          for k in range(x.shape[2]):\n",
        "              _[j][k] = torch.min(_[j][k], x[j,i,:][k])\n",
        "  \n",
        "  return _\n",
        "\n",
        "\n",
        "def get_span_tensor(bert_t, index, last_layer=L, pad_id=-1):\n",
        "    \"\"\"return Seq X Layer X Feat\"\"\"\n",
        "    span_tensor = []\n",
        "    for i in index:\n",
        "        if i == pad_id:\n",
        "            break\n",
        "        span_tensor.append(bert_t[-last_layer:, i, :])\n",
        "    return torch.stack(span_tensor)\n",
        "    \n",
        "_ = GPTData(all_data)\n",
        "gpt_iter = DataLoader(_, batch_size=5, sampler=SortLenSampler(_, 'encode'), collate_fn=gpt_collate_func)\n",
        "\n",
        "## Extract BERT features\n",
        "bert_feats = []\n",
        "print('extract bert features..')\n",
        "bert.eval()\n",
        "for (x, p, a, b, y, id_) in gpt_iter:\n",
        "    r = bert.forward(x.cuda(), attention_mask= (x!=0).cuda())\n",
        "    # _ = torch.stack(r[0][-L:]).cpu().data.clone() ## For BERT - last L layers\n",
        "    # _ = torch.stack(r[2][-L:]).cpu().data.clone() ## For RoBERTa - last L layers\n",
        "    # _ = torch.stack([r[0][-1]] + [r[0][0]]).cpu().data.clone() ## top+bottom\n",
        "    # _ = torch.stack(r[0][16:21]).cpu().data.clone() ## For BERT - Layers 16-20\n",
        "    _ = torch.stack(r[2][16:21]).cpu().data.clone() ## For RoBERTa - Layers 16-20\n",
        "\n",
        "    del(r)\n",
        "    for i, v in enumerate(id_):\n",
        "        bert_feats.append({'a': get_span_tensor(_[:,i,:],a[i]),\n",
        "                           'b': get_span_tensor(_[:,i,:],b[i]),\n",
        "                           'p': meanpooling(_[:,i,:], p[i]),\n",
        "                           'ap': (a[i][0] - p[i][0]).type(torch.FloatTensor),\n",
        "                           'bp': (b[i][0] - p[i][0]).type(torch.FloatTensor),\n",
        "                           'y': y[i],\n",
        "                           'id': v})\n",
        "\n",
        "print('extract bert features finished.')       \n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\n",
        "############\n",
        "\n",
        "class BERTfeature(Dataset):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "    \n",
        "def bert_collate_func(x):\n",
        "    _ = [[] for i in range(6)]\n",
        "    for i in x:\n",
        "        _[0].append(i['a'])\n",
        "        _[1].append(i['b'])\n",
        "        _[2].append(i['p'])\n",
        "        _[3].append(i['y'])\n",
        "        _[4].append(i['ap'])\n",
        "        _[5].append(i['bp'])\n",
        "    return [pad_sequence(v, batch_first=True) if i < 2 else torch.stack(v) for i, v in enumerate(_)]\n",
        "\n",
        "## Split into train and test\n",
        "test = [i for i in bert_feats if 'dev' in i['id']]\n",
        "train = [i for i in bert_feats if 'dev' not in i['id']]\n",
        "\n",
        "#############\n",
        "\n",
        "def get_mask(t, shape=(8,123), padding_value=0):\n",
        "    \"\"\"input padded batch input B X Seq X Layer X Feats, output mask with shape BXMask \"\"\"\n",
        "    if padding_value != 0:\n",
        "        raise ValueError\n",
        "    padding_value = torch.zeros(shape)\n",
        "    if t.is_cuda:\n",
        "        padding_value = padding_value.cuda()\n",
        "    mask = []\n",
        "    for i in t:\n",
        "        _ = torch.zeros(i.shape[0])\n",
        "        if t.is_cuda:\n",
        "            _ = _.cuda()\n",
        "        for j in range(i.shape[0]):\n",
        "            if (i[j] == padding_value).sum()==shape[0]*shape[1]:\n",
        "                break\n",
        "            _[j] = 1\n",
        "        mask.append(_)\n",
        "    return torch.stack(mask)\n",
        "\n",
        "def masked_softmax(vec, mask, dim=1, epsilon=1e-15):\n",
        "    exps = torch.exp(vec)\n",
        "    masked_exps = exps * mask\n",
        "    masked_sums = masked_exps.sum(dim, keepdim=True) + epsilon\n",
        "    return masked_exps/masked_sums\n",
        "\n",
        "\n",
        "class AttentionSimilarityLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_dim, dropout=0.3):\n",
        "        super(AttentionSimilarityLayer, self).__init__()\n",
        "        self.ffnn = nn.Linear(hidden_dim*5, S_DIM)\n",
        "        nn.init.kaiming_normal_(self.ffnn.weight)\n",
        "        self.ln = nn.LayerNorm(hidden_dim, elementwise_affine=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.repr_dropout = nn.Dropout(0.4)\n",
        "        self.rescale = 1/np.sqrt(hidden_dim)\n",
        "\n",
        "    def forward(self, a, a_mask, b, b_mask, p):\n",
        "        a = self.ln(a)\n",
        "        b = self.ln(b)\n",
        "        a_s = (self.repr_dropout(a) @ p[:,:,None]) * self.rescale\n",
        "        b_s = (self.repr_dropout(b) @ p[:,:,None]) * self.rescale\n",
        "        a_attn = masked_softmax(a_s.squeeze(2), a_mask, dim=1)\n",
        "        b_attn = masked_softmax(b_s.squeeze(2), b_mask, dim=1)\n",
        "        a = (a * a_attn[:,:,None]).sum(1)\n",
        "        b = (b * b_attn[:,:,None]).sum(1)\n",
        "        _input = torch.cat([p, a, b, p*a, p*b], dim=1)\n",
        "        y = self.ffnn(self.dropout(_input))\n",
        "\n",
        "        return y\n",
        "    \n",
        "\n",
        "class MSnet(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_dim, dropout=0.5, hidden_layer=4):\n",
        "        super(MSnet, self).__init__()\n",
        "        self.sim_layers = nn.ModuleList([AttentionSimilarityLayer(hidden_dim, dropout=dropout) for i in range(hidden_layer)])\n",
        "        self.bn = nn.BatchNorm1d(S_DIM*hidden_layer)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.mention_score = nn.Linear(S_DIM*hidden_layer+2, 3)\n",
        "        self.dist_ecoding = nn.Linear(1,1)\n",
        "        \n",
        "    def forward(self, a, b, p, ap, bp):\n",
        "        y = []\n",
        "        a_mask = get_mask(a, shape=a.shape[2:])\n",
        "        b_mask = get_mask(b, shape=b.shape[2:])\n",
        "        for i, l in enumerate(self.sim_layers):\n",
        "            y.append(l(a[:,:,i,:], a_mask, b[:,:,i,:], b_mask, p[:,i,:]))\n",
        "        y = torch.cat(y, dim=1) # B X 64*Layer\n",
        "        y = self.dropout(self.bn(y).relu())\n",
        "        ap = self.dist_ecoding(ap[:,None]).tanh()\n",
        "        bp = self.dist_ecoding(bp[:,None]).tanh()\n",
        "        return self.mention_score(torch.cat([y, ap, bp], dim=1))\n",
        "\n",
        "\n",
        "def training_cuda(epoch, model, lossfunc, optimizer, train_iter, val_iter, test_iter, start=5):\n",
        "    best_score = 10\n",
        "    for i in range(epoch):\n",
        "        model.train()\n",
        "        epoch_score = np.array([])\n",
        "        for (a, b, p, y, ap, bp) in iter(train_iter):\n",
        "            model.zero_grad()\n",
        "            pred = model.forward(a.cuda(), b.cuda(), p.cuda(), ap.cuda(), bp.cuda())\n",
        "            # loss = lossfunc(pred, y.cuda()) + l2 * torch.stack([torch.norm(i[1]) for i in model.named_parameters() if 'weight' in i[0]]).sum()\n",
        "            loss = lossfunc(pred, y.cuda())\n",
        "            s = score(pred.softmax(1), y.cuda())\n",
        "            epoch_score = np.append(epoch_score, s.cpu().data.numpy())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            model.zero_grad()\n",
        "            val_score =  np.array([])\n",
        "            for (va, vb, vp, vy, vap, vbp) in val_iter:\n",
        "                vpred = model.forward(va.cuda(), vb.cuda(), vp.cuda(), vap.cuda(), vbp.cuda())\n",
        "                vs = score(vpred.softmax(1), vy.cuda())\n",
        "                val_score = np.append(val_score, vs.cpu().data.numpy())\n",
        "            print('epcoh {:02} - train_score {:.4f} - val_score {:.4f} '.format(\n",
        "                                i, np.mean(epoch_score), np.mean(val_score)))\n",
        "            if  np.mean(val_score) < best_score:\n",
        "                best_score = np.mean(val_score)\n",
        "                if i > start:\n",
        "                    torch.save(model.state_dict(), 'tmp.m')\n",
        "    model.load_state_dict(torch.load('tmp.m'))\n",
        "    test_pred = np.array([])\n",
        "    for (ta, tb, tp, ty, tap, tbp) in test_iter:\n",
        "        vpred = model.forward(ta.cuda(), tb.cuda(), tp.cuda(), tap.cuda(), tbp.cuda())\n",
        "        test_pred = np.append(test_pred, vpred.softmax(1).cpu().data.numpy())\n",
        "    return best_score, test_pred\n",
        "\n",
        "\n",
        "def score(pred, y):\n",
        "    t_float = torch.FloatTensor\n",
        "    if isinstance(pred, torch.cuda.FloatTensor):\n",
        "        t_float = torch.cuda.FloatTensor\n",
        "    y = (torch.cumsum(torch.ones(y.shape[0], 3), dim=1) -1).type(t_float) == y[:,None].type(t_float)\n",
        "    s = (y.type(t_float) * pred).sum(1).log()\n",
        "    return -s\n",
        "\n",
        "## Training \n",
        "print('training')\n",
        "m = MSnet(BERT_SIZE, dropout=0.4, hidden_layer=L).cuda()\n",
        "optimizer = optim.Adam(m.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "loss_fuc = nn.CrossEntropyLoss()\n",
        "batch_size = 32\n",
        "\n",
        "kfold = KFold(n_splits=5, random_state=SEED, shuffle=True)\n",
        "scores = []\n",
        "m_s = deepcopy(m.state_dict().copy())\n",
        "opt_s = deepcopy(optimizer.state_dict().copy())\n",
        "\n",
        "k_th = 0\n",
        "test_iter = DataLoader(BERTfeature(test), batch_size=batch_size, shuffle=False, collate_fn=bert_collate_func)\n",
        "test_preds = []\n",
        "\n",
        "for train_idx, val_idx in kfold.split(list(range(len(train)))):\n",
        "    \n",
        "    _train = [v for i, v in enumerate(train) if i in train_idx]\n",
        "    _val = [v for i, v in enumerate(train) if i in val_idx]\n",
        "    train_iter = DataLoader(BERTfeature(_train), batch_size=batch_size, shuffle=True, collate_fn=bert_collate_func)\n",
        "    val_iter = DataLoader(BERTfeature(_val), batch_size=batch_size, shuffle=False, collate_fn=bert_collate_func)\n",
        "    \n",
        "    m.load_state_dict(m_s)\n",
        "    optimizer.load_state_dict(opt_s)\n",
        "    s, y = training_cuda(30, m, loss_fuc, optimizer, train_iter, val_iter, test_iter)\n",
        "    scores.append(s)\n",
        "    test_preds.append(y)\n",
        "    \n",
        "    k_th += 1\n",
        "    print('------------'*3)\n",
        "    \n",
        "print('Score: {:.4f} {:.4f}'.format(np.mean(scores), np.std(scores)))\n",
        "probs = np.mean(test_preds, axis=0).reshape((-1, 3))\n",
        "true = torch.cat([ty for (ta, tb, tp, ty, tap, tbp) in test_iter], dim=0).data.numpy()\n",
        "t_ids = [i['id'] for i in test]\n",
        "print(log_loss(true, probs))\n",
        "\n",
        "## Accuracy & F1 score\n",
        "acc_pred = []\n",
        "for i in range(len(probs)):\n",
        "    acc_pred.append(list(probs[i]).index(max(probs[i])))\n",
        "acc_pred = np.asarray(acc_pred)\n",
        "\n",
        "print(\"Accuracy:\",accuracy_score(true, acc_pred))\n",
        "print(\"F1 score:\",f1_score(true, acc_pred, average='micro'))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenize...\n",
            "clean..\n",
            "extract bert features..\n",
            "extract bert features finished.\n",
            "training\n",
            "epcoh 00 - train_score 1.0113 - val_score 0.9244 \n",
            "epcoh 01 - train_score 0.8095 - val_score 0.7438 \n",
            "epcoh 02 - train_score 0.6813 - val_score 0.6321 \n",
            "epcoh 03 - train_score 0.5872 - val_score 0.5564 \n",
            "epcoh 04 - train_score 0.5192 - val_score 0.4795 \n",
            "epcoh 05 - train_score 0.4554 - val_score 0.4291 \n",
            "epcoh 06 - train_score 0.4059 - val_score 0.3985 \n",
            "epcoh 07 - train_score 0.3661 - val_score 0.3868 \n",
            "epcoh 08 - train_score 0.3271 - val_score 0.3648 \n",
            "epcoh 09 - train_score 0.3002 - val_score 0.3334 \n",
            "epcoh 10 - train_score 0.2770 - val_score 0.3282 \n",
            "epcoh 11 - train_score 0.2533 - val_score 0.3313 \n",
            "epcoh 12 - train_score 0.2373 - val_score 0.3194 \n",
            "epcoh 13 - train_score 0.2069 - val_score 0.3240 \n",
            "epcoh 14 - train_score 0.1959 - val_score 0.3313 \n",
            "epcoh 15 - train_score 0.1818 - val_score 0.3129 \n",
            "epcoh 16 - train_score 0.1693 - val_score 0.3312 \n",
            "epcoh 17 - train_score 0.1512 - val_score 0.3165 \n",
            "epcoh 18 - train_score 0.1531 - val_score 0.3259 \n",
            "epcoh 19 - train_score 0.1502 - val_score 0.3378 \n",
            "epcoh 20 - train_score 0.1315 - val_score 0.3428 \n",
            "epcoh 21 - train_score 0.1170 - val_score 0.3404 \n",
            "epcoh 22 - train_score 0.1108 - val_score 0.3379 \n",
            "epcoh 23 - train_score 0.1056 - val_score 0.3384 \n",
            "epcoh 24 - train_score 0.0953 - val_score 0.3444 \n",
            "epcoh 25 - train_score 0.0924 - val_score 0.3373 \n",
            "epcoh 26 - train_score 0.0947 - val_score 0.3333 \n",
            "epcoh 27 - train_score 0.0863 - val_score 0.3486 \n",
            "epcoh 28 - train_score 0.0757 - val_score 0.3455 \n",
            "epcoh 29 - train_score 0.0684 - val_score 0.3582 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 1.0022 - val_score 0.8908 \n",
            "epcoh 01 - train_score 0.8081 - val_score 0.7378 \n",
            "epcoh 02 - train_score 0.6777 - val_score 0.6202 \n",
            "epcoh 03 - train_score 0.6017 - val_score 0.5415 \n",
            "epcoh 04 - train_score 0.4933 - val_score 0.4775 \n",
            "epcoh 05 - train_score 0.4426 - val_score 0.4380 \n",
            "epcoh 06 - train_score 0.3871 - val_score 0.4103 \n",
            "epcoh 07 - train_score 0.3459 - val_score 0.3892 \n",
            "epcoh 08 - train_score 0.3076 - val_score 0.3738 \n",
            "epcoh 09 - train_score 0.2819 - val_score 0.3620 \n",
            "epcoh 10 - train_score 0.2616 - val_score 0.3743 \n",
            "epcoh 11 - train_score 0.2261 - val_score 0.3699 \n",
            "epcoh 12 - train_score 0.2068 - val_score 0.3608 \n",
            "epcoh 13 - train_score 0.1941 - val_score 0.3644 \n",
            "epcoh 14 - train_score 0.1866 - val_score 0.3652 \n",
            "epcoh 15 - train_score 0.1694 - val_score 0.3830 \n",
            "epcoh 16 - train_score 0.1579 - val_score 0.3666 \n",
            "epcoh 17 - train_score 0.1544 - val_score 0.3722 \n",
            "epcoh 18 - train_score 0.1242 - val_score 0.3790 \n",
            "epcoh 19 - train_score 0.1273 - val_score 0.3803 \n",
            "epcoh 20 - train_score 0.1082 - val_score 0.3940 \n",
            "epcoh 21 - train_score 0.1131 - val_score 0.3902 \n",
            "epcoh 22 - train_score 0.1095 - val_score 0.3960 \n",
            "epcoh 23 - train_score 0.1010 - val_score 0.4267 \n",
            "epcoh 24 - train_score 0.0793 - val_score 0.4173 \n",
            "epcoh 25 - train_score 0.1050 - val_score 0.4044 \n",
            "epcoh 26 - train_score 0.0764 - val_score 0.4245 \n",
            "epcoh 27 - train_score 0.0783 - val_score 0.4252 \n",
            "epcoh 28 - train_score 0.0915 - val_score 0.4363 \n",
            "epcoh 29 - train_score 0.0677 - val_score 0.4192 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 1.0022 - val_score 0.9168 \n",
            "epcoh 01 - train_score 0.8128 - val_score 0.7546 \n",
            "epcoh 02 - train_score 0.6784 - val_score 0.6597 \n",
            "epcoh 03 - train_score 0.5743 - val_score 0.5767 \n",
            "epcoh 04 - train_score 0.4971 - val_score 0.5122 \n",
            "epcoh 05 - train_score 0.4316 - val_score 0.4892 \n",
            "epcoh 06 - train_score 0.3791 - val_score 0.4545 \n",
            "epcoh 07 - train_score 0.3292 - val_score 0.4409 \n",
            "epcoh 08 - train_score 0.3234 - val_score 0.4114 \n",
            "epcoh 09 - train_score 0.2853 - val_score 0.4012 \n",
            "epcoh 10 - train_score 0.2550 - val_score 0.3915 \n",
            "epcoh 11 - train_score 0.2400 - val_score 0.3948 \n",
            "epcoh 12 - train_score 0.2027 - val_score 0.3788 \n",
            "epcoh 13 - train_score 0.2000 - val_score 0.3792 \n",
            "epcoh 14 - train_score 0.1720 - val_score 0.3882 \n",
            "epcoh 15 - train_score 0.1774 - val_score 0.3646 \n",
            "epcoh 16 - train_score 0.1698 - val_score 0.3853 \n",
            "epcoh 17 - train_score 0.1554 - val_score 0.3685 \n",
            "epcoh 18 - train_score 0.1473 - val_score 0.3889 \n",
            "epcoh 19 - train_score 0.1256 - val_score 0.3901 \n",
            "epcoh 20 - train_score 0.1159 - val_score 0.3885 \n",
            "epcoh 21 - train_score 0.1099 - val_score 0.3917 \n",
            "epcoh 22 - train_score 0.0970 - val_score 0.3901 \n",
            "epcoh 23 - train_score 0.1017 - val_score 0.3906 \n",
            "epcoh 24 - train_score 0.0946 - val_score 0.4037 \n",
            "epcoh 25 - train_score 0.0791 - val_score 0.3925 \n",
            "epcoh 26 - train_score 0.0881 - val_score 0.4041 \n",
            "epcoh 27 - train_score 0.0846 - val_score 0.4104 \n",
            "epcoh 28 - train_score 0.0778 - val_score 0.4151 \n",
            "epcoh 29 - train_score 0.0726 - val_score 0.4134 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 1.0069 - val_score 0.9162 \n",
            "epcoh 01 - train_score 0.8117 - val_score 0.7509 \n",
            "epcoh 02 - train_score 0.6752 - val_score 0.6437 \n",
            "epcoh 03 - train_score 0.5839 - val_score 0.5468 \n",
            "epcoh 04 - train_score 0.5116 - val_score 0.5200 \n",
            "epcoh 05 - train_score 0.4431 - val_score 0.4366 \n",
            "epcoh 06 - train_score 0.3992 - val_score 0.4116 \n",
            "epcoh 07 - train_score 0.3559 - val_score 0.3906 \n",
            "epcoh 08 - train_score 0.3175 - val_score 0.3797 \n",
            "epcoh 09 - train_score 0.2768 - val_score 0.3762 \n",
            "epcoh 10 - train_score 0.2594 - val_score 0.3701 \n",
            "epcoh 11 - train_score 0.2518 - val_score 0.3676 \n",
            "epcoh 12 - train_score 0.2371 - val_score 0.3697 \n",
            "epcoh 13 - train_score 0.2099 - val_score 0.3607 \n",
            "epcoh 14 - train_score 0.1761 - val_score 0.3605 \n",
            "epcoh 15 - train_score 0.1836 - val_score 0.3622 \n",
            "epcoh 16 - train_score 0.1740 - val_score 0.3654 \n",
            "epcoh 17 - train_score 0.1553 - val_score 0.3657 \n",
            "epcoh 18 - train_score 0.1367 - val_score 0.3760 \n",
            "epcoh 19 - train_score 0.1326 - val_score 0.3781 \n",
            "epcoh 20 - train_score 0.1264 - val_score 0.3756 \n",
            "epcoh 21 - train_score 0.1111 - val_score 0.3766 \n",
            "epcoh 22 - train_score 0.0951 - val_score 0.3961 \n",
            "epcoh 23 - train_score 0.1113 - val_score 0.3910 \n",
            "epcoh 24 - train_score 0.1018 - val_score 0.3817 \n",
            "epcoh 25 - train_score 0.0865 - val_score 0.4125 \n",
            "epcoh 26 - train_score 0.0704 - val_score 0.4018 \n",
            "epcoh 27 - train_score 0.0721 - val_score 0.4098 \n",
            "epcoh 28 - train_score 0.0933 - val_score 0.4157 \n",
            "epcoh 29 - train_score 0.0760 - val_score 0.4234 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 1.0094 - val_score 0.8907 \n",
            "epcoh 01 - train_score 0.8175 - val_score 0.7593 \n",
            "epcoh 02 - train_score 0.6676 - val_score 0.6043 \n",
            "epcoh 03 - train_score 0.5720 - val_score 0.5390 \n",
            "epcoh 04 - train_score 0.5024 - val_score 0.4840 \n",
            "epcoh 05 - train_score 0.4316 - val_score 0.4525 \n",
            "epcoh 06 - train_score 0.3731 - val_score 0.4376 \n",
            "epcoh 07 - train_score 0.3482 - val_score 0.4228 \n",
            "epcoh 08 - train_score 0.3071 - val_score 0.3963 \n",
            "epcoh 09 - train_score 0.2650 - val_score 0.3755 \n",
            "epcoh 10 - train_score 0.2488 - val_score 0.3878 \n",
            "epcoh 11 - train_score 0.2443 - val_score 0.3773 \n",
            "epcoh 12 - train_score 0.2209 - val_score 0.3793 \n",
            "epcoh 13 - train_score 0.2050 - val_score 0.4003 \n",
            "epcoh 14 - train_score 0.1901 - val_score 0.3740 \n",
            "epcoh 15 - train_score 0.1749 - val_score 0.3972 \n",
            "epcoh 16 - train_score 0.1581 - val_score 0.3939 \n",
            "epcoh 17 - train_score 0.1471 - val_score 0.4032 \n",
            "epcoh 18 - train_score 0.1306 - val_score 0.4122 \n",
            "epcoh 19 - train_score 0.1114 - val_score 0.4039 \n",
            "epcoh 20 - train_score 0.1090 - val_score 0.4273 \n",
            "epcoh 21 - train_score 0.1112 - val_score 0.4244 \n",
            "epcoh 22 - train_score 0.0989 - val_score 0.4326 \n",
            "epcoh 23 - train_score 0.0850 - val_score 0.4351 \n",
            "epcoh 24 - train_score 0.0860 - val_score 0.4570 \n",
            "epcoh 25 - train_score 0.0773 - val_score 0.4329 \n",
            "epcoh 26 - train_score 0.0772 - val_score 0.4436 \n",
            "epcoh 27 - train_score 0.0803 - val_score 0.4567 \n",
            "epcoh 28 - train_score 0.0712 - val_score 0.4629 \n",
            "epcoh 29 - train_score 0.0666 - val_score 0.4747 \n",
            "------------------------------------\n",
            "Score: 0.3546 0.0214\n",
            "0.3327365655294299\n",
            "Accuracy: 0.886\n",
            "F1 score: 0.886\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}