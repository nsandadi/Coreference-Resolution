{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PoolingWithAttention_DPR.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aAPdfuzU5tVU"
      ],
      "authorship_tag": "ABX9TyN2zi8P2dy2sa1bSEbtBZ4O",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsandadi/Coreference-Resolution/blob/main/PoolingWithAttention_DPR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHHNQ7JwhgJo"
      },
      "source": [
        "## Meanpooling with Attention (DPR)\n",
        "### BERT, RoBERTa, CorefRoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inYvfQDbhsUA"
      },
      "source": [
        "## Install Transformers library from Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT2LelAsRXpZ",
        "outputId": "d1700925-3d3a-4316-efaf-1229c1a292f4"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-7vwgaxlm\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-7vwgaxlm\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.8.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 34.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2090005 sha256=f7fec769468e93970e79d65ab216d597d0135939120f2a789dcc3d2f87e18f90\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jkc02p3_/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "Successfully built transformers\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=79adad7912a1ae4a4a505984423968d2d66881da779101ec3073e118a2af44e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.6.0.dev0\n",
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 8.1MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/79/64c0815cbe8c6abd7fe5525ec37a2689d3cf10e387629ba4a6e44daff6d0/boto3-1.17.49-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.49\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/59/6e28ce58206039ad2592992b75ee79a8f9dbc902a9704373ddacc4f96300/botocore-1.20.49-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 11.9MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.49->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.49->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.20.49 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.17.49 botocore-1.20.49 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgqGVknsh278"
      },
      "source": [
        "## Import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvwm9_oYRc8z",
        "outputId": "8a6e6740-44f7-453f-e20e-9a4ea3b19313"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, BatchSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import re\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import timeit\n",
        "\n",
        "\n",
        "print('installing apex')\n",
        "os.system('git clone -q https://github.com/NVIDIA/apex.git')\n",
        "os.system('pip install -q --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex/')\n",
        "os.system('rm -rf apex')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "installing apex\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyNCvdnXh8-Y"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-YczPmxSpIL",
        "outputId": "873984fb-3ecb-4eae-9b81-20b2206f1904"
      },
      "source": [
        "from google.colab import drive\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "drive.mount('/contents/')\n",
        "\n",
        "# Load data from drive\n",
        "winograd_train = pd.read_csv('/contents/My Drive/Winograd/winograd_train.csv')\n",
        "winograd_val = pd.read_csv('/contents/My Drive/Winograd/winograd_dev.csv')\n",
        "winograd_test = pd.read_csv('/contents/My Drive/Winograd/winograd_test.csv')\n",
        "\n",
        "print(\"Training data has\", len(winograd_train), \"records.\")\n",
        "print(\"Validation data has\", len(winograd_val), \"records.\")\n",
        "print(\"Test data has\", len(winograd_test), \"records.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /contents/\n",
            "Training data has 564 records.\n",
            "Validation data has 282 records.\n",
            "Test data has 282 records.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPvOX4GF0i4E"
      },
      "source": [
        "# ### Combining winograd dataset to test set\n",
        "# gender_pronouns = ['she','her','hers','She','Her','Hers','he','him','his','He','Him','His']\n",
        "# winograd_test = winograd_test[winograd_test.Pronoun.astype(str).isin(gender_pronouns)]\n",
        "# winograd_train = winograd_train[winograd_train.Pronoun.astype(str).isin(gender_pronouns)]\n",
        "# winograd_val = winograd_val[winograd_val.Pronoun.astype(str).isin(gender_pronouns)]\n",
        "# winograd_data = pd.concat([winograd_train, winograd_val, winograd_test])\n",
        "# winograd_data = winograd_data.reset_index(drop=True)\n",
        "# winograd_data['ID'] = winograd_data.index \n",
        "# winograd_data['ID'] = winograd_data.ID+1\n",
        "# winograd_data['ID'] = 'test-' + winograd_data['ID'].astype(str)\n",
        "# print(\"Dataset has\",len(winograd_data),\"records\")\n",
        "# winograd_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv26Yjrks4aX"
      },
      "source": [
        "# # winograd_test = winograd_test[winograd_test.Pronoun.astype(str).isin(['she','her','hers','She','Her','Hers'])]\n",
        "# winograd_test = winograd_test[winograd_test.Pronoun.astype(str).isin(['he','his','him','He','His','Him'])]\n",
        "# winograd_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKWrEnJ3S6Ia",
        "outputId": "f42e6f93-c754-4d26-97eb-894a1087efa4"
      },
      "source": [
        "winograd_train['ID'] = winograd_train.index \n",
        "winograd_train['ID'] = winograd_train.ID+1\n",
        "winograd_train['ID'] = 'development-' + winograd_train['ID'].astype(str)\n",
        "\n",
        "winograd_val['ID'] = winograd_val.index \n",
        "winograd_val['ID'] = winograd_val.ID+1\n",
        "winograd_val['ID'] = 'validation-' + winograd_val['ID'].astype(str)\n",
        "\n",
        "winograd_test['ID'] = winograd_test.index \n",
        "winograd_test['ID'] = winograd_test.ID+1\n",
        "winograd_test['ID'] = 'test-' + winograd_test['ID'].astype(str)\n",
        "\n",
        "print(\"Training data has\", len(winograd_train), \"records.\")\n",
        "print(\"Validation data has\", len(winograd_val), \"records.\")\n",
        "print(\"Test data has\", len(winograd_test), \"records.\")\n",
        "\n",
        "# Concatenate train, validation and test datasets\n",
        "winograd_data = pd.concat([winograd_train, winograd_val, winograd_test])\n",
        "winograd_data = winograd_data.reset_index(drop=True)\n",
        "print(\"Winograd data has\", len(winograd_data), \"records.\")\n",
        "\n",
        "# Reorder columns\n",
        "cols = winograd_data.columns.tolist()\n",
        "cols = cols[-1:] + cols[:-1]\n",
        "winograd_data = winograd_data[cols]\n",
        "winograd_data.head()\n",
        "all_data = winograd_data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data has 564 records.\n",
            "Validation data has 282 records.\n",
            "Test data has 282 records.\n",
            "Winograd data has 1128 records.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8PmuDv7wHkV",
        "outputId": "6777d2d3-6360-4a8f-9583-26fdbb45286e"
      },
      "source": [
        "os.system('wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-development.tsv -q')\n",
        "os.system('wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-test.tsv -q')\n",
        "os.system('wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-validation.tsv -q')\n",
        "\n",
        "gap_dev = pd.read_csv('gap-development.tsv', delimiter='\\t')\n",
        "gap_val = pd.read_csv('gap-validation.tsv', delimiter='\\t')\n",
        "gap_test = pd.read_csv('gap-test.tsv', delimiter='\\t')\n",
        "\n",
        "# all_data = pd.concat([gap_dev, gap_val, winograd_data])\n",
        "# all_data = all_data.reset_index(drop=True)\n",
        "\n",
        "# print(\"Dataset contains\", len(all_data),\"records\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset contains 1128 records\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlAIOa7DiA40"
      },
      "source": [
        "## Download pre-trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrNPYngI3Bxl"
      },
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, WordpieceTokenizer\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "######## hyper-parameters tuning ######\n",
        "# BERT_NAME = 'bert-large-uncased'\n",
        "BERT_NAME = 'roberta-large'\n",
        "# BERT_NAME = 'nielsr/coref-roberta-large'\n",
        "BERT_SIZE = 1024  \n",
        "SEED = 23\n",
        "L = 5\n",
        "S_DIM = 256\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # If model is BERT\n",
        "# tokenizer = BertTokenizer.from_pretrained(BERT_NAME)\n",
        "# bert = BertModel.from_pretrained(BERT_NAME).cuda()\n",
        "# bert = bert.to(device)\n",
        "\n",
        "# If model is RoBERTa\n",
        "tokenizer = RobertaTokenizer.from_pretrained(BERT_NAME)\n",
        "bert = RobertaModel.from_pretrained(BERT_NAME, output_hidden_states=True).cuda()\n",
        "\n",
        "# # If model is CorefBERT or CorefRoBERTa\n",
        "# from transformers import AutoModel, AutoTokenizer\n",
        "# bert = AutoModel.from_pretrained(BERT_NAME, output_hidden_states= True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(BERT_NAME)\n",
        "# bert = bert.to(device)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAPdfuzU5tVU"
      },
      "source": [
        "## Meanpooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v15slWy5s6A",
        "outputId": "01b324f3-769f-4974-d9ad-f3f0b65df0b2"
      },
      "source": [
        "def bert_tokenize(text, p, a, b, p_offset, a_offset, b_offset):\n",
        "    idxs = {}\n",
        "    tokens = []\n",
        "    \n",
        "    a_span = [a_offset, a_offset+len(a), 'a']\n",
        "    b_span = [b_offset, b_offset+len(b), 'b']\n",
        "    p_span = [p_offset, p_offset+len(p), 'p']\n",
        "    \n",
        "    spans = [a_span, b_span, p_span]\n",
        "    spans = sorted(spans, key=lambda x: x[0])\n",
        "    \n",
        "    last_offset = 0\n",
        "    idx = -1\n",
        "    \n",
        "    def token_part(string):\n",
        "        _idxs = []\n",
        "        nonlocal idx\n",
        "        for w in tokenizer.tokenize(string):\n",
        "            idx += 1\n",
        "            tokens.append(w)\n",
        "            _idxs.append(idx)\n",
        "        return _idxs\n",
        "    \n",
        "    \n",
        "    for span in spans:\n",
        "        token_part(text[last_offset:span[0]])\n",
        "        idxs[span[2]] = token_part(text[span[0]:span[1]])\n",
        "        last_offset = span[1]\n",
        "    token_part(text[last_offset:])\n",
        "    return tokens, idxs\n",
        "\n",
        "print('tokenize...')\n",
        "_ = all_data.apply(lambda x: bert_tokenize(x['Text'], x['Pronoun'], x['A'], x['B'], x['Pronoun-offset'], x['A-offset'], x['B-offset']), axis=1)\n",
        "all_data['encode'] = [tokenizer.convert_tokens_to_ids(i[0]) for i in _]\n",
        "all_data['p_idx'] = [i[1]['p'] for i in _]\n",
        "all_data['a_idx'] = [i[1]['a'] for i in _]\n",
        "all_data['b_idx'] = [i[1]['b'] for i in _]\n",
        "\n",
        "class GPTData(Dataset):\n",
        "    \n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        _ = self.data.loc[idx]\n",
        "        sample = {'id': _['ID'],\n",
        "                  'encode': torch.LongTensor([101] + _['encode'] + [102]),\n",
        "                  'p_idx': torch.LongTensor(_['p_idx'])+1,\n",
        "                  'a_idx': torch.LongTensor(_['a_idx'])+1,\n",
        "                  'b_idx': torch.LongTensor(_['b_idx'])+1,\n",
        "                  'coref': torch.LongTensor([0 if _['A-coref'] else 1 if _['B-coref'] else 2])\n",
        "                 }\n",
        "        return sample\n",
        "        \n",
        "class SortLenSampler(Sampler):\n",
        "    \n",
        "    def __init__(self, data_source, key):\n",
        "        self.sorted_idx = sorted(range(len(data_source)), key=lambda x: len(data_source[x][key]))\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return iter(self.sorted_idx)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sorted_idx)\n",
        "        \n",
        "\n",
        "def gpt_collate_func(x):\n",
        "    _ = [[], [], [], [], [], []]\n",
        "    for i in x:\n",
        "        _[0].append(i['encode'])\n",
        "        _[1].append(i['p_idx'])\n",
        "        _[2].append(i['a_idx'])\n",
        "        _[3].append(i['b_idx'])\n",
        "        _[4].append(i['coref'])\n",
        "        _[5].append(i['id'])\n",
        "    return torch.nn.utils.rnn.pad_sequence(_[0], batch_first=True, padding_value=0), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(_[1], batch_first=True, padding_value=-1), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(_[2], batch_first=True, padding_value=-1), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(_[3], batch_first=True, padding_value=-1), \\\n",
        "           torch.cat(_[4], dim=0), _[5]\n",
        "\n",
        "def meanpooling(x, idx, pad=-1):\n",
        "    \"\"\"x: Layer X Seq X Feat, idx: Seq \"\"\"\n",
        "    t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "    _ = torch.zeros((x.shape[0], x.shape[2]))\n",
        "    cnt = 0\n",
        "    for i in idx:\n",
        "        if i == pad:\n",
        "            break\n",
        "        for j in range(x.shape[0]):\n",
        "            _[j] += x[j,i,:]\n",
        "        cnt += 1\n",
        "    \n",
        "    if cnt == 0:\n",
        "        raise ValueError('0 divide')\n",
        "    mean = _/cnt\n",
        "    \n",
        "    return mean\n",
        "\n",
        "def sumpooling(x, idx, pad=-1):\n",
        "    \"\"\"x: Layer X Seq X Feat, idx: Seq \"\"\"\n",
        "    t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "    _ = torch.zeros((x.shape[0], x.shape[2]))\n",
        "    cnt = 0\n",
        "    for i in idx:\n",
        "        if i == pad:\n",
        "            break\n",
        "        for j in range(x.shape[0]):\n",
        "            _[j] += x[j,i,:]\n",
        "    \n",
        "    return _\n",
        "\n",
        "def maxpooling(x, idx, pad=-1):\n",
        "  \"\"\"x: Layer X Seq X Feat, idx: Seq\"\"\"\n",
        "  t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "  _ = torch.full((x.shape[0], x.shape[2]), -float('inf'))\n",
        "  for i in idx:\n",
        "      if i == pad:\n",
        "          break\n",
        "      for j in range(x.shape[0]):\n",
        "          for k in range(x.shape[2]):\n",
        "              _[j][k] = torch.max(_[j][k], x[j,i,:][k])\n",
        "  \n",
        "  return _\n",
        "\n",
        "\n",
        "def minpooling(x, idx, pad=-1):\n",
        "  \"\"\"x: Layer X Seq X Feat, idx: Seq\"\"\"\n",
        "  t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "  _ = torch.full((x.shape[0], x.shape[2]), float('inf'))\n",
        "  for i in idx:\n",
        "      if i == pad:\n",
        "          break\n",
        "      for j in range(x.shape[0]):\n",
        "          for k in range(x.shape[2]):\n",
        "              _[j][k] = torch.min(_[j][k], x[j,i,:][k])\n",
        "  \n",
        "  return _\n",
        "\n",
        "\n",
        "def get_span_tensor(bert_t, index, last_layer=L, pad_id=-1):\n",
        "    \"\"\"return Seq X Layer X Feat\"\"\"\n",
        "    span_tensor = []\n",
        "    for i in index:\n",
        "        if i == pad_id:\n",
        "            break\n",
        "        span_tensor.append(bert_t[16:21, i, :])\n",
        "        # span_tensor.append(bert_t[-last_layer:, i, :])\n",
        "    return torch.stack(span_tensor)\n",
        "    \n",
        "\n",
        "_ = GPTData(all_data)\n",
        "gpt_iter = DataLoader(_, batch_size=5, sampler=SortLenSampler(_, 'encode'), collate_fn=gpt_collate_func)\n",
        "\n",
        "bert_feats = []\n",
        "print('extract bert features..')\n",
        "start = timeit.default_timer()\n",
        "bert.eval()\n",
        "for (x, p, a, b, y, id_) in gpt_iter:\n",
        "    r = bert.forward(x.cuda(), attention_mask= (x!=0).cuda())\n",
        "    _ = torch.stack(r[2][16:21]).cpu().data.clone()\n",
        "    # _ = torch.stack(r[2][-L:]).cpu().data.clone()  \n",
        "    del(r)\n",
        "    for i, v in enumerate(id_):\n",
        "        bert_feats.append({'a': meanpooling(_[:,i,:],a[i]),\n",
        "                           'b': meanpooling(_[:,i,:],b[i]),\n",
        "                           'p': meanpooling(_[:,i,:],p[i]),\n",
        "                           'ap': (a[i][0] - p[i][0]).type(torch.FloatTensor),\n",
        "                           'bp': (b[i][0] - p[i][0]).type(torch.FloatTensor),\n",
        "                           'y': y[i],\n",
        "                           'id': v})\n",
        "\n",
        "print('extract bert features finished.')\n",
        "stop = timeit.default_timer()\n",
        "print('Runtime: ', stop - start)        \n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "############\n",
        "\n",
        "class BERTfeature(Dataset):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "    \n",
        "def bert_collate_func(x):\n",
        "    _ = [[] for i in range(6)]\n",
        "    for i in x:\n",
        "        _[0].append(i['a'])\n",
        "        _[1].append(i['b'])\n",
        "        _[2].append(i['p'])\n",
        "        _[3].append(i['y'])\n",
        "        _[4].append(i['ap'])\n",
        "        _[5].append(i['bp'])\n",
        "    return [torch.stack(i) for i in _]\n",
        "\n",
        "############\n",
        "\n",
        "test = [i for i in bert_feats if 'test' in i['id']]\n",
        "train = [i for i in bert_feats if 'test' not in i['id']]\n",
        "\n",
        "############\n",
        "class SimilarityLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_dim, dropout=0.3):\n",
        "        super(SimilarityLayer, self).__init__()\n",
        "        self.ffnn = nn.Linear(hidden_dim*5, S_DIM)\n",
        "        nn.init.kaiming_normal_(self.ffnn.weight)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, a, b, p):\n",
        "        _input = torch.cat([p, a, b, p*a, p*b], dim=1)\n",
        "        y = self.ffnn(self.dropout(_input))\n",
        "        \n",
        "        return y\n",
        "    \n",
        "\n",
        "class MSnet(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_dim, dropout=0.5, hidden_layer=4):\n",
        "        super(MSnet, self).__init__()\n",
        "        self.sim_layers = nn.ModuleList([SimilarityLayer(hidden_dim, dropout=dropout) for i in range(hidden_layer)])\n",
        "        self.bn = nn.BatchNorm1d(S_DIM*hidden_layer)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.mention_score = nn.Linear(S_DIM*hidden_layer+2, 3)\n",
        "        self.dist_ecoding = nn.Linear(1,1)\n",
        "        \n",
        "    def forward(self, a, b, p, ap, bp):\n",
        "        y = []\n",
        "        for i, l in enumerate(self.sim_layers):\n",
        "            y.append(l(a[:,i,:], b[:,i,:], p[:,i,:]))\n",
        "        y = torch.cat(y, dim=1) # B X 64*Layer\n",
        "        y = self.dropout(self.bn(y).relu())\n",
        "        ap = self.dist_ecoding(ap[:,None]).tanh()\n",
        "        bp = self.dist_ecoding(bp[:,None]).tanh()\n",
        "        return self.mention_score(torch.cat([y, ap, bp], dim=1))\n",
        "\n",
        "\n",
        "def training_cuda(epoch, model, lossfunc, optimizer, train_iter, val_iter, test_iter, start=5):\n",
        "    best_score = 10\n",
        "    for i in range(epoch):\n",
        "        model.train()\n",
        "        epoch_score = np.array([])\n",
        "        for (a, b, p, y, ap, bp) in iter(train_iter):\n",
        "            model.zero_grad()\n",
        "            pred = model.forward(a.cuda(), b.cuda(), p.cuda(), ap.cuda(), bp.cuda())\n",
        "            # loss = lossfunc(pred, y.cuda()) + l2 * torch.stack([torch.norm(i[1]) for i in model.named_parameters() if 'weight' in i[0]]).sum()\n",
        "            loss = lossfunc(pred, y.cuda())\n",
        "            s = score(pred.softmax(1), y.cuda())\n",
        "            epoch_score = np.append(epoch_score, s.cpu().data.numpy())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            model.zero_grad()\n",
        "            val_score =  np.array([])\n",
        "            for (va, vb, vp, vy, vap, vbp) in val_iter:\n",
        "                vpred = model.forward(va.cuda(), vb.cuda(), vp.cuda(), vap.cuda(), vbp.cuda())\n",
        "                vs = score(vpred.softmax(1), vy.cuda())\n",
        "                val_score = np.append(val_score, vs.cpu().data.numpy())\n",
        "            print('epcoh {:02} - train_score {:.4f} - val_score {:.4f} '.format(\n",
        "                                i, np.mean(epoch_score), np.mean(val_score)))\n",
        "            if  np.mean(val_score) < best_score:\n",
        "                best_score = np.mean(val_score)\n",
        "                if i > start:\n",
        "                    torch.save(model.state_dict(), 'tmp.m')\n",
        "    model.load_state_dict(torch.load('tmp.m'))\n",
        "    test_pred = np.array([])\n",
        "    for (ta, tb, tp, ty, tap, tbp) in test_iter:\n",
        "        vpred = model.forward(ta.cuda(), tb.cuda(), tp.cuda(), tap.cuda(), tbp.cuda())\n",
        "        test_pred = np.append(test_pred, vpred.softmax(1).cpu().data.numpy())\n",
        "    return best_score, test_pred\n",
        "\n",
        "\n",
        "def score(pred, y):\n",
        "    t_float = torch.FloatTensor\n",
        "    if isinstance(pred, torch.cuda.FloatTensor):\n",
        "        t_float = torch.cuda.FloatTensor\n",
        "    y = (torch.cumsum(torch.ones(y.shape[0], 3), dim=1) -1).type(t_float) == y[:,None].type(t_float)\n",
        "    s = (y.type(t_float) * pred).sum(1).log()\n",
        "    return -s\n",
        "\n",
        "## Training\n",
        "print('training')\n",
        "m = MSnet(BERT_SIZE, dropout=0.6, hidden_layer=L).cuda()\n",
        "optimizer = optim.Adam(m.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "loss_fuc = nn.CrossEntropyLoss()\n",
        "batch_size = 32\n",
        "\n",
        "kfold = KFold(n_splits=5, random_state=SEED, shuffle=True)\n",
        "scores = []\n",
        "m_s = deepcopy(m.state_dict().copy())\n",
        "opt_s = deepcopy(optimizer.state_dict().copy())\n",
        "\n",
        "k_th = 0\n",
        "test_iter = DataLoader(BERTfeature(test), batch_size=batch_size, shuffle=False, collate_fn=bert_collate_func)\n",
        "test_preds = []\n",
        "\n",
        "for train_idx, val_idx in kfold.split(list(range(len(train)))):\n",
        "    \n",
        "    _train = [v for i, v in enumerate(train) if i in train_idx]\n",
        "    _val = [v for i, v in enumerate(train) if i in val_idx]\n",
        "    train_iter = DataLoader(BERTfeature(_train), batch_size=batch_size, shuffle=True, collate_fn=bert_collate_func)\n",
        "    val_iter = DataLoader(BERTfeature(_val), batch_size=batch_size, shuffle=False, collate_fn=bert_collate_func)\n",
        "    \n",
        "    m.load_state_dict(m_s)\n",
        "    optimizer.load_state_dict(opt_s)\n",
        "    s, y = training_cuda(30, m, loss_fuc, optimizer, train_iter, val_iter, test_iter)\n",
        "    scores.append(s)\n",
        "    test_preds.append(y)\n",
        "    \n",
        "    k_th += 1\n",
        "    print('------------'*3)\n",
        "    \n",
        "print('Score: {:.4f} {:.4f}'.format(np.mean(scores), np.std(scores)))\n",
        "probs = np.mean(test_preds, axis=0).reshape((-1, 3))\n",
        "true = torch.cat([ty for (ta, tb, tp, ty, tap, tbp) in test_iter], dim=0).data.numpy()\n",
        "t_ids = [i['id'] for i in test]\n",
        "print(log_loss(true, probs))\n",
        "\n",
        "## Accuracy\n",
        "acc_pred = []\n",
        "for i in range(len(probs)):\n",
        "    acc_pred.append(list(probs[i]).index(max(probs[i])))\n",
        "acc_pred = np.asarray(acc_pred)\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"Accuracy:\",accuracy_score(true, acc_pred))\n",
        "print(\"Accuracy:\",f1_score(true, acc_pred, average='micro'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenize...\n",
            "extract bert features..\n",
            "extract bert features finished.\n",
            "Runtime:  5.780389404000005\n",
            "training\n",
            "epcoh 00 - train_score 1.1421 - val_score 1.0751 \n",
            "epcoh 01 - train_score 0.9886 - val_score 1.0542 \n",
            "epcoh 02 - train_score 0.8714 - val_score 1.0403 \n",
            "epcoh 03 - train_score 0.8307 - val_score 1.0067 \n",
            "epcoh 04 - train_score 0.8121 - val_score 1.0001 \n",
            "epcoh 05 - train_score 0.7748 - val_score 0.9735 \n",
            "epcoh 06 - train_score 0.7871 - val_score 0.9819 \n",
            "epcoh 07 - train_score 0.7424 - val_score 0.9385 \n",
            "epcoh 08 - train_score 0.7634 - val_score 0.9404 \n",
            "epcoh 09 - train_score 0.7345 - val_score 0.9228 \n",
            "epcoh 10 - train_score 0.7371 - val_score 0.9390 \n",
            "epcoh 11 - train_score 0.7236 - val_score 0.9264 \n",
            "epcoh 12 - train_score 0.7056 - val_score 0.8891 \n",
            "epcoh 13 - train_score 0.7206 - val_score 0.8762 \n",
            "epcoh 14 - train_score 0.6998 - val_score 0.8673 \n",
            "epcoh 15 - train_score 0.7077 - val_score 0.8546 \n",
            "epcoh 16 - train_score 0.6786 - val_score 0.8589 \n",
            "epcoh 17 - train_score 0.6729 - val_score 0.7963 \n",
            "epcoh 18 - train_score 0.6534 - val_score 0.8147 \n",
            "epcoh 19 - train_score 0.6424 - val_score 0.7951 \n",
            "epcoh 20 - train_score 0.6404 - val_score 0.7944 \n",
            "epcoh 21 - train_score 0.6041 - val_score 0.7707 \n",
            "epcoh 22 - train_score 0.6134 - val_score 0.7408 \n",
            "epcoh 23 - train_score 0.5966 - val_score 0.7295 \n",
            "epcoh 24 - train_score 0.6039 - val_score 0.7232 \n",
            "epcoh 25 - train_score 0.5940 - val_score 0.7251 \n",
            "epcoh 26 - train_score 0.5743 - val_score 0.6848 \n",
            "epcoh 27 - train_score 0.5637 - val_score 0.6805 \n",
            "epcoh 28 - train_score 0.5432 - val_score 0.6879 \n",
            "epcoh 29 - train_score 0.5604 - val_score 0.6758 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 1.1530 - val_score 1.0728 \n",
            "epcoh 01 - train_score 0.9697 - val_score 1.0652 \n",
            "epcoh 02 - train_score 0.8768 - val_score 1.0025 \n",
            "epcoh 03 - train_score 0.8659 - val_score 0.9946 \n",
            "epcoh 04 - train_score 0.7991 - val_score 0.9914 \n",
            "epcoh 05 - train_score 0.7891 - val_score 0.9814 \n",
            "epcoh 06 - train_score 0.7974 - val_score 0.9593 \n",
            "epcoh 07 - train_score 0.7679 - val_score 0.9504 \n",
            "epcoh 08 - train_score 0.7343 - val_score 0.9516 \n",
            "epcoh 09 - train_score 0.7355 - val_score 0.9363 \n",
            "epcoh 10 - train_score 0.7355 - val_score 0.9247 \n",
            "epcoh 11 - train_score 0.7308 - val_score 0.9230 \n",
            "epcoh 12 - train_score 0.7064 - val_score 0.9049 \n",
            "epcoh 13 - train_score 0.7187 - val_score 0.8952 \n",
            "epcoh 14 - train_score 0.6880 - val_score 0.8815 \n",
            "epcoh 15 - train_score 0.6628 - val_score 0.8736 \n",
            "epcoh 16 - train_score 0.6812 - val_score 0.8307 \n",
            "epcoh 17 - train_score 0.6475 - val_score 0.8396 \n",
            "epcoh 18 - train_score 0.6586 - val_score 0.8054 \n",
            "epcoh 19 - train_score 0.6360 - val_score 0.7952 \n",
            "epcoh 20 - train_score 0.6494 - val_score 0.8183 \n",
            "epcoh 21 - train_score 0.6411 - val_score 0.7969 \n",
            "epcoh 22 - train_score 0.5951 - val_score 0.7615 \n",
            "epcoh 23 - train_score 0.5688 - val_score 0.7488 \n",
            "epcoh 24 - train_score 0.5854 - val_score 0.7342 \n",
            "epcoh 25 - train_score 0.5613 - val_score 0.7253 \n",
            "epcoh 26 - train_score 0.5521 - val_score 0.7271 \n",
            "epcoh 27 - train_score 0.5372 - val_score 0.7095 \n",
            "epcoh 28 - train_score 0.5023 - val_score 0.6839 \n",
            "epcoh 29 - train_score 0.5111 - val_score 0.6918 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 1.1388 - val_score 1.0681 \n",
            "epcoh 01 - train_score 0.9641 - val_score 1.0577 \n",
            "epcoh 02 - train_score 0.8531 - val_score 1.0257 \n",
            "epcoh 03 - train_score 0.8437 - val_score 1.0190 \n",
            "epcoh 04 - train_score 0.8152 - val_score 0.9885 \n",
            "epcoh 05 - train_score 0.8024 - val_score 0.9872 \n",
            "epcoh 06 - train_score 0.7749 - val_score 0.9823 \n",
            "epcoh 07 - train_score 0.7637 - val_score 0.9699 \n",
            "epcoh 08 - train_score 0.7505 - val_score 0.9623 \n",
            "epcoh 09 - train_score 0.7237 - val_score 0.9337 \n",
            "epcoh 10 - train_score 0.7203 - val_score 0.9458 \n",
            "epcoh 11 - train_score 0.6984 - val_score 0.9139 \n",
            "epcoh 12 - train_score 0.7045 - val_score 0.9173 \n",
            "epcoh 13 - train_score 0.7072 - val_score 0.9033 \n",
            "epcoh 14 - train_score 0.6835 - val_score 0.8902 \n",
            "epcoh 15 - train_score 0.6914 - val_score 0.8510 \n",
            "epcoh 16 - train_score 0.6429 - val_score 0.8630 \n",
            "epcoh 17 - train_score 0.6464 - val_score 0.8298 \n",
            "epcoh 18 - train_score 0.6416 - val_score 0.8416 \n",
            "epcoh 19 - train_score 0.6425 - val_score 0.8110 \n",
            "epcoh 20 - train_score 0.6165 - val_score 0.8022 \n",
            "epcoh 21 - train_score 0.5918 - val_score 0.7755 \n",
            "epcoh 22 - train_score 0.6002 - val_score 0.7835 \n",
            "epcoh 23 - train_score 0.5945 - val_score 0.7704 \n",
            "epcoh 24 - train_score 0.5672 - val_score 0.7617 \n",
            "epcoh 25 - train_score 0.5372 - val_score 0.7347 \n",
            "epcoh 26 - train_score 0.5436 - val_score 0.7510 \n",
            "epcoh 27 - train_score 0.5171 - val_score 0.7309 \n",
            "epcoh 28 - train_score 0.5324 - val_score 0.7165 \n",
            "epcoh 29 - train_score 0.4929 - val_score 0.7092 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 1.1178 - val_score 1.0836 \n",
            "epcoh 01 - train_score 0.9683 - val_score 1.0400 \n",
            "epcoh 02 - train_score 0.8854 - val_score 1.0395 \n",
            "epcoh 03 - train_score 0.8369 - val_score 1.0136 \n",
            "epcoh 04 - train_score 0.8072 - val_score 0.9979 \n",
            "epcoh 05 - train_score 0.7774 - val_score 0.9972 \n",
            "epcoh 06 - train_score 0.7614 - val_score 0.9716 \n",
            "epcoh 07 - train_score 0.7705 - val_score 0.9664 \n",
            "epcoh 08 - train_score 0.7588 - val_score 0.9415 \n",
            "epcoh 09 - train_score 0.7313 - val_score 0.9378 \n",
            "epcoh 10 - train_score 0.7338 - val_score 0.9540 \n",
            "epcoh 11 - train_score 0.7076 - val_score 0.9323 \n",
            "epcoh 12 - train_score 0.7054 - val_score 0.9043 \n",
            "epcoh 13 - train_score 0.6830 - val_score 0.9014 \n",
            "epcoh 14 - train_score 0.6929 - val_score 0.8752 \n",
            "epcoh 15 - train_score 0.6694 - val_score 0.8875 \n",
            "epcoh 16 - train_score 0.6679 - val_score 0.8640 \n",
            "epcoh 17 - train_score 0.6534 - val_score 0.8425 \n",
            "epcoh 18 - train_score 0.6268 - val_score 0.8222 \n",
            "epcoh 19 - train_score 0.6434 - val_score 0.7889 \n",
            "epcoh 20 - train_score 0.6245 - val_score 0.8002 \n",
            "epcoh 21 - train_score 0.5999 - val_score 0.7770 \n",
            "epcoh 22 - train_score 0.5931 - val_score 0.7696 \n",
            "epcoh 23 - train_score 0.5763 - val_score 0.7830 \n",
            "epcoh 24 - train_score 0.5614 - val_score 0.7322 \n",
            "epcoh 25 - train_score 0.5570 - val_score 0.7375 \n",
            "epcoh 26 - train_score 0.5334 - val_score 0.7058 \n",
            "epcoh 27 - train_score 0.5300 - val_score 0.7306 \n",
            "epcoh 28 - train_score 0.5026 - val_score 0.7265 \n",
            "epcoh 29 - train_score 0.5198 - val_score 0.7216 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 1.1588 - val_score 1.0692 \n",
            "epcoh 01 - train_score 0.9604 - val_score 1.0511 \n",
            "epcoh 02 - train_score 0.8737 - val_score 1.0308 \n",
            "epcoh 03 - train_score 0.8451 - val_score 1.0063 \n",
            "epcoh 04 - train_score 0.7917 - val_score 1.0049 \n",
            "epcoh 05 - train_score 0.7880 - val_score 0.9880 \n",
            "epcoh 06 - train_score 0.7697 - val_score 0.9589 \n",
            "epcoh 07 - train_score 0.7614 - val_score 0.9716 \n",
            "epcoh 08 - train_score 0.7476 - val_score 0.9515 \n",
            "epcoh 09 - train_score 0.7383 - val_score 0.9341 \n",
            "epcoh 10 - train_score 0.7233 - val_score 0.9230 \n",
            "epcoh 11 - train_score 0.7086 - val_score 0.9183 \n",
            "epcoh 12 - train_score 0.7141 - val_score 0.8902 \n",
            "epcoh 13 - train_score 0.6970 - val_score 0.8906 \n",
            "epcoh 14 - train_score 0.6753 - val_score 0.8919 \n",
            "epcoh 15 - train_score 0.6677 - val_score 0.8719 \n",
            "epcoh 16 - train_score 0.6848 - val_score 0.8370 \n",
            "epcoh 17 - train_score 0.6547 - val_score 0.8370 \n",
            "epcoh 18 - train_score 0.6370 - val_score 0.8340 \n",
            "epcoh 19 - train_score 0.6465 - val_score 0.8059 \n",
            "epcoh 20 - train_score 0.6180 - val_score 0.7939 \n",
            "epcoh 21 - train_score 0.6007 - val_score 0.7918 \n",
            "epcoh 22 - train_score 0.6066 - val_score 0.7886 \n",
            "epcoh 23 - train_score 0.5854 - val_score 0.7587 \n",
            "epcoh 24 - train_score 0.5634 - val_score 0.7446 \n",
            "epcoh 25 - train_score 0.5412 - val_score 0.7410 \n",
            "epcoh 26 - train_score 0.5159 - val_score 0.7408 \n",
            "epcoh 27 - train_score 0.5564 - val_score 0.7157 \n",
            "epcoh 28 - train_score 0.5263 - val_score 0.7189 \n",
            "epcoh 29 - train_score 0.5265 - val_score 0.7065 \n",
            "------------------------------------\n",
            "Score: 0.6962 0.0137\n",
            "0.6158942496356876\n",
            "Accuracy: 0.7836879432624113\n",
            "Accuracy: 0.7836879432624113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7DQqrPo5pND"
      },
      "source": [
        "## Pooling with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rYhciBG1XZx",
        "outputId": "42e6d8c3-8cd4-4a25-8615-597560a62227"
      },
      "source": [
        "## Tokenize\n",
        "def bert_tokenize(text, p, a, b, p_offset, a_offset, b_offset):\n",
        "    idxs = {}\n",
        "    tokens = []\n",
        "    \n",
        "    a_span = [a_offset, a_offset+len(a), 'a']\n",
        "    b_span = [b_offset, b_offset+len(b), 'b']\n",
        "    p_span = [p_offset, p_offset+len(p), 'p']\n",
        "    \n",
        "    spans = [a_span, b_span, p_span]\n",
        "    spans = sorted(spans, key=lambda x: x[0])\n",
        "    \n",
        "    last_offset = 0\n",
        "    idx = -1\n",
        "    \n",
        "    def token_part(string):\n",
        "        _idxs = []\n",
        "        nonlocal idx\n",
        "        for w in tokenizer.tokenize(string):\n",
        "            idx += 1\n",
        "            tokens.append(w)\n",
        "            _idxs.append(idx)\n",
        "        return _idxs\n",
        "    \n",
        "    \n",
        "    for span in spans:\n",
        "        token_part(text[last_offset:span[0]])\n",
        "        idxs[span[2]] = token_part(text[span[0]:span[1]])\n",
        "        last_offset = span[1]\n",
        "    token_part(text[last_offset:])\n",
        "    return tokens, idxs\n",
        "    \n",
        "\n",
        "print('tokenize...')\n",
        "_ = all_data.apply(lambda x: bert_tokenize(x['Text'], x['Pronoun'], x['A'], x['B'], x['Pronoun-offset'], x['A-offset'], x['B-offset']), axis=1)\n",
        "all_data['encode'] = [tokenizer.convert_tokens_to_ids(i[0]) for i in _]\n",
        "all_data['p_idx'] = [i[1]['p'] for i in _]\n",
        "all_data['a_idx'] = [i[1]['a'] for i in _]\n",
        "all_data['b_idx'] = [i[1]['b'] for i in _]\n",
        "\n",
        "##########\n",
        "\n",
        "class GPTData(Dataset):\n",
        "    \n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        _ = self.data.loc[idx]\n",
        "        sample = {'id': _['ID'],\n",
        "                  'encode': torch.LongTensor([101] + _['encode'] + [102]),\n",
        "                  'p_idx': torch.LongTensor(_['p_idx'])+1,\n",
        "                  'a_idx': torch.LongTensor(_['a_idx'])+1,\n",
        "                  'b_idx': torch.LongTensor(_['b_idx'])+1,\n",
        "                  'coref': torch.LongTensor([0 if _['A-coref'] else 1 if _['B-coref'] else 2])\n",
        "                 }\n",
        "        return sample\n",
        "        \n",
        "class SortLenSampler(Sampler):\n",
        "    \n",
        "    def __init__(self, data_source, key):\n",
        "        self.sorted_idx = sorted(range(len(data_source)), key=lambda x: len(data_source[x][key]))\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return iter(self.sorted_idx)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sorted_idx)\n",
        "        \n",
        "\n",
        "def gpt_collate_func(x):\n",
        "    _ = [[], [], [], [], [], []]\n",
        "    for i in x:\n",
        "        _[0].append(i['encode'])\n",
        "        _[1].append(i['p_idx'])\n",
        "        _[2].append(i['a_idx'])\n",
        "        _[3].append(i['b_idx'])\n",
        "        _[4].append(i['coref'])\n",
        "        _[5].append(i['id'])\n",
        "    return torch.nn.utils.rnn.pad_sequence(_[0], batch_first=True, padding_value=0), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(_[1], batch_first=True, padding_value=-1), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(_[2], batch_first=True, padding_value=-1), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(_[3], batch_first=True, padding_value=-1), \\\n",
        "           torch.cat(_[4], dim=0), _[5]\n",
        "\n",
        "\n",
        "## Pooling\n",
        "def meanpooling(x, idx, pad=-1):\n",
        "    \"\"\"x: Layer X Seq X Feat, idx: Seq \"\"\"\n",
        "    t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "    \n",
        "    _ = torch.zeros((x.shape[0], x.shape[2]))\n",
        "    cnt = 0\n",
        "    for i in idx:\n",
        "        if i == pad:\n",
        "            break\n",
        "        for j in range(x.shape[0]):\n",
        "            _[j] += x[j,i,:]\n",
        "        cnt += 1\n",
        "    if cnt == 0:\n",
        "        raise ValueError('0 dive')\n",
        "    return _/cnt\n",
        "\n",
        "def maxpooling(x, idx, pad=-1):\n",
        "  \"\"\"x: Layer X Seq X Feat, idx: Seq\"\"\"\n",
        "  t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "  _ = torch.full((x.shape[0], x.shape[2]), -float('inf'))\n",
        "  for i in idx:\n",
        "      if i == pad:\n",
        "          break\n",
        "      for j in range(x.shape[0]):\n",
        "          for k in range(x.shape[2]):\n",
        "              _[j][k] = torch.max(_[j][k], x[j,i,:][k])\n",
        "  \n",
        "  return _\n",
        "\n",
        "\n",
        "def minpooling(x, idx, pad=-1):\n",
        "  \"\"\"x: Layer X Seq X Feat, idx: Seq\"\"\"\n",
        "  t_type = torch.cuda.FloatTensor if isinstance(x, torch.cuda.FloatTensor) else torch.FloatTensor\n",
        "  _ = torch.full((x.shape[0], x.shape[2]), float('inf'))\n",
        "  for i in idx:\n",
        "      if i == pad:\n",
        "          break\n",
        "      for j in range(x.shape[0]):\n",
        "          for k in range(x.shape[2]):\n",
        "              _[j][k] = torch.min(_[j][k], x[j,i,:][k])\n",
        "  \n",
        "  return _\n",
        "\n",
        "\n",
        "def get_span_tensor(bert_t, index, last_layer=L, pad_id=-1):\n",
        "    \"\"\"return Seq X Layer X Feat\"\"\"\n",
        "    span_tensor = []\n",
        "    for i in index:\n",
        "        if i == pad_id:\n",
        "            break\n",
        "        span_tensor.append(bert_t[-last_layer:, i, :])\n",
        "    return torch.stack(span_tensor)\n",
        "    \n",
        "_ = GPTData(all_data)\n",
        "gpt_iter = DataLoader(_, batch_size=5, sampler=SortLenSampler(_, 'encode'), collate_fn=gpt_collate_func)\n",
        "\n",
        "## Extract BERT features\n",
        "bert_feats = []\n",
        "print('extract bert features..')\n",
        "bert.eval()\n",
        "for (x, p, a, b, y, id_) in gpt_iter:\n",
        "    r = bert.forward(x.cuda(), attention_mask= (x!=0).cuda())\n",
        "    # _ = torch.stack(r[0][-L:]).cpu().data.clone() ## For BERT - last L layers\n",
        "    # _ = torch.stack(r[2][-L:]).cpu().data.clone() ## For RoBERTa - last L layers\n",
        "    # _ = torch.stack([r[0][-1]] + [r[0][0]]).cpu().data.clone() ## top+bottom\n",
        "    _ = torch.stack(r[2][16:21]).cpu().data.clone() ## Layer 16-20\n",
        "    del(r)\n",
        "    for i, v in enumerate(id_):\n",
        "        bert_feats.append({'a': get_span_tensor(_[:,i,:],a[i]),\n",
        "                           'b': get_span_tensor(_[:,i,:],b[i]),\n",
        "                           'p': meanpooling(_[:,i,:], p[i]),\n",
        "                           'ap': (a[i][0] - p[i][0]).type(torch.FloatTensor),\n",
        "                           'bp': (b[i][0] - p[i][0]).type(torch.FloatTensor),\n",
        "                           'y': y[i],\n",
        "                           'id': v})\n",
        "\n",
        "print('extract bert features finished.')       \n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "########\n",
        "\n",
        "class BERTfeature(Dataset):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "    \n",
        "def bert_collate_func(x):\n",
        "    _ = [[] for i in range(6)]\n",
        "    for i in x:\n",
        "        _[0].append(i['a'])\n",
        "        _[1].append(i['b'])\n",
        "        _[2].append(i['p'])\n",
        "        _[3].append(i['y'])\n",
        "        _[4].append(i['ap'])\n",
        "        _[5].append(i['bp'])\n",
        "    return [pad_sequence(v, batch_first=True) if i < 2 else torch.stack(v) for i, v in enumerate(_)]\n",
        "\n",
        "\n",
        "## Split into train and test set\n",
        "test = [i for i in bert_feats if 'test' in i['id']]\n",
        "train = [i for i in bert_feats if 'test' not in i['id']]\n",
        "\n",
        "\n",
        "## Attention Mechanism\n",
        "def get_mask(t, shape=(8,123), padding_value=0):\n",
        "    \"\"\"input padded batch input B X Seq X Layer X Feats, output mask with shape BXMask \"\"\"\n",
        "    if padding_value != 0:\n",
        "        raise ValueError\n",
        "    padding_value = torch.zeros(shape)\n",
        "    if t.is_cuda:\n",
        "        padding_value = padding_value.cuda()\n",
        "    mask = []\n",
        "    for i in t:\n",
        "        _ = torch.zeros(i.shape[0])\n",
        "        if t.is_cuda:\n",
        "            _ = _.cuda()\n",
        "        for j in range(i.shape[0]):\n",
        "            if (i[j] == padding_value).sum()==shape[0]*shape[1]:\n",
        "                break\n",
        "            _[j] = 1\n",
        "        mask.append(_)\n",
        "    return torch.stack(mask)\n",
        "\n",
        "def masked_softmax(vec, mask, dim=1, epsilon=1e-15):\n",
        "    exps = torch.exp(vec)\n",
        "    masked_exps = exps * mask\n",
        "    masked_sums = masked_exps.sum(dim, keepdim=True) + epsilon\n",
        "    return masked_exps/masked_sums\n",
        "\n",
        "\n",
        "class AttentionSimilarityLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_dim, dropout=0.3):\n",
        "        super(AttentionSimilarityLayer, self).__init__()\n",
        "        self.ffnn = nn.Linear(hidden_dim*5, S_DIM)\n",
        "        nn.init.kaiming_normal_(self.ffnn.weight)\n",
        "        self.ln = nn.LayerNorm(hidden_dim, elementwise_affine=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.repr_dropout = nn.Dropout(0.4)\n",
        "        self.rescale = 1/np.sqrt(hidden_dim)\n",
        "\n",
        "    def forward(self, a, a_mask, b, b_mask, p):\n",
        "        a = self.ln(a)\n",
        "        b = self.ln(b)\n",
        "        a_s = (self.repr_dropout(a) @ p[:,:,None]) * self.rescale\n",
        "        b_s = (self.repr_dropout(b) @ p[:,:,None]) * self.rescale\n",
        "        a_attn = masked_softmax(a_s.squeeze(2), a_mask, dim=1)\n",
        "        b_attn = masked_softmax(b_s.squeeze(2), b_mask, dim=1)\n",
        "        a = (a * a_attn[:,:,None]).sum(1)\n",
        "        b = (b * b_attn[:,:,None]).sum(1)\n",
        "        _input = torch.cat([p, a, b, p*a, p*b], dim=1)\n",
        "        y = self.ffnn(self.dropout(_input))\n",
        "\n",
        "        return y\n",
        "    \n",
        "\n",
        "class MSnet(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_dim, dropout=0.5, hidden_layer=4):\n",
        "        super(MSnet, self).__init__()\n",
        "        self.sim_layers = nn.ModuleList([AttentionSimilarityLayer(hidden_dim, dropout=dropout) for i in range(hidden_layer)])\n",
        "        self.bn = nn.BatchNorm1d(S_DIM*hidden_layer)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.mention_score = nn.Linear(S_DIM*hidden_layer+2, 3)\n",
        "        self.dist_ecoding = nn.Linear(1,1)\n",
        "        \n",
        "    def forward(self, a, b, p, ap, bp):\n",
        "        y = []\n",
        "        a_mask = get_mask(a, shape=a.shape[2:])\n",
        "        b_mask = get_mask(b, shape=b.shape[2:])\n",
        "        for i, l in enumerate(self.sim_layers):\n",
        "            y.append(l(a[:,:,i,:], a_mask, b[:,:,i,:], b_mask, p[:,i,:]))\n",
        "        y = torch.cat(y, dim=1) # B X 64*Layer\n",
        "        y = self.dropout(self.bn(y).relu())\n",
        "        ap = self.dist_ecoding(ap[:,None]).tanh()\n",
        "        bp = self.dist_ecoding(bp[:,None]).tanh()\n",
        "        return self.mention_score(torch.cat([y, ap, bp], dim=1))\n",
        "\n",
        "\n",
        "def training_cuda(epoch, model, lossfunc, optimizer, train_iter, val_iter, test_iter, start=5):\n",
        "    best_score = 10\n",
        "    for i in range(epoch):\n",
        "        model.train()\n",
        "        epoch_score = np.array([])\n",
        "        for (a, b, p, y, ap, bp) in iter(train_iter):\n",
        "            model.zero_grad()\n",
        "            pred = model.forward(a.cuda(), b.cuda(), p.cuda(), ap.cuda(), bp.cuda())\n",
        "            # loss = lossfunc(pred, y.cuda()) + l2 * torch.stack([torch.norm(i[1]) for i in model.named_parameters() if 'weight' in i[0]]).sum()\n",
        "            loss = lossfunc(pred, y.cuda())\n",
        "            s = score(pred.softmax(1), y.cuda())\n",
        "            epoch_score = np.append(epoch_score, s.cpu().data.numpy())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            model.zero_grad()\n",
        "            val_score =  np.array([])\n",
        "            for (va, vb, vp, vy, vap, vbp) in val_iter:\n",
        "                vpred = model.forward(va.cuda(), vb.cuda(), vp.cuda(), vap.cuda(), vbp.cuda())\n",
        "                vs = score(vpred.softmax(1), vy.cuda())\n",
        "                val_score = np.append(val_score, vs.cpu().data.numpy())\n",
        "            print('epcoh {:02} - train_score {:.4f} - val_score {:.4f} '.format(\n",
        "                                i, np.mean(epoch_score), np.mean(val_score)))\n",
        "            if  np.mean(val_score) < best_score:\n",
        "                best_score = np.mean(val_score)\n",
        "                if i > start:\n",
        "                    torch.save(model.state_dict(), 'tmp.m')\n",
        "    model.load_state_dict(torch.load('tmp.m'))\n",
        "    test_pred = np.array([])\n",
        "    for (ta, tb, tp, ty, tap, tbp) in test_iter:\n",
        "        vpred = model.forward(ta.cuda(), tb.cuda(), tp.cuda(), tap.cuda(), tbp.cuda())\n",
        "        test_pred = np.append(test_pred, vpred.softmax(1).cpu().data.numpy())\n",
        "    return best_score, test_pred\n",
        "\n",
        "\n",
        "def score(pred, y):\n",
        "    t_float = torch.FloatTensor\n",
        "    if isinstance(pred, torch.cuda.FloatTensor):\n",
        "        t_float = torch.cuda.FloatTensor\n",
        "    y = (torch.cumsum(torch.ones(y.shape[0], 3), dim=1) -1).type(t_float) == y[:,None].type(t_float)\n",
        "    s = (y.type(t_float) * pred).sum(1).log()\n",
        "    return -s\n",
        "\n",
        "\n",
        "## Training\n",
        "print('training')\n",
        "m = MSnet(BERT_SIZE, dropout=0.4, hidden_layer=L).cuda()\n",
        "optimizer = optim.Adam(m.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "loss_fuc = nn.CrossEntropyLoss()\n",
        "batch_size = 32\n",
        "\n",
        "kfold = KFold(n_splits=5, random_state=SEED, shuffle=True)\n",
        "scores = []\n",
        "m_s = deepcopy(m.state_dict().copy())\n",
        "opt_s = deepcopy(optimizer.state_dict().copy())\n",
        "\n",
        "k_th = 0\n",
        "test_iter = DataLoader(BERTfeature(test), batch_size=batch_size, shuffle=False, collate_fn=bert_collate_func)\n",
        "test_preds = []\n",
        "\n",
        "for train_idx, val_idx in kfold.split(list(range(len(train)))):\n",
        "    \n",
        "    _train = [v for i, v in enumerate(train) if i in train_idx]\n",
        "    _val = [v for i, v in enumerate(train) if i in val_idx]\n",
        "    train_iter = DataLoader(BERTfeature(_train), batch_size=batch_size, shuffle=True, collate_fn=bert_collate_func)\n",
        "    val_iter = DataLoader(BERTfeature(_val), batch_size=batch_size, shuffle=False, collate_fn=bert_collate_func)\n",
        "    \n",
        "    m.load_state_dict(m_s)\n",
        "    optimizer.load_state_dict(opt_s)\n",
        "    s, y = training_cuda(30, m, loss_fuc, optimizer, train_iter, val_iter, test_iter)\n",
        "    scores.append(s)\n",
        "    test_preds.append(y)\n",
        "    \n",
        "    k_th += 1\n",
        "    print('------------'*3)\n",
        "    \n",
        "print('Score: {:.4f} {:.4f}'.format(np.mean(scores), np.std(scores)))\n",
        "probs = np.mean(test_preds, axis=0).reshape((-1, 3))\n",
        "true = torch.cat([ty for (ta, tb, tp, ty, tap, tbp) in test_iter], dim=0).data.numpy()\n",
        "t_ids = [i['id'] for i in test]\n",
        "print(log_loss(true, probs))\n",
        "\n",
        "## Accuracy\n",
        "acc_pred = []\n",
        "for i in range(len(probs)):\n",
        "    acc_pred.append(list(probs[i]).index(max(probs[i])))\n",
        "acc_pred = np.asarray(acc_pred)\n",
        "\n",
        "print(\"Accuracy:\",accuracy_score(true, acc_pred))\n",
        "print(\"F1 score:\",f1_score(true, acc_pred, average='micro'))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenize...\n",
            "extract bert features..\n",
            "extract bert features finished.\n",
            "training\n",
            "epcoh 00 - train_score 0.8589 - val_score 0.8712 \n",
            "epcoh 01 - train_score 0.6985 - val_score 0.8653 \n",
            "epcoh 02 - train_score 0.6583 - val_score 0.8452 \n",
            "epcoh 03 - train_score 0.6676 - val_score 0.8068 \n",
            "epcoh 04 - train_score 0.6333 - val_score 0.7777 \n",
            "epcoh 05 - train_score 0.5992 - val_score 0.7538 \n",
            "epcoh 06 - train_score 0.5696 - val_score 0.7305 \n",
            "epcoh 07 - train_score 0.5397 - val_score 0.6765 \n",
            "epcoh 08 - train_score 0.5148 - val_score 0.6475 \n",
            "epcoh 09 - train_score 0.4941 - val_score 0.6630 \n",
            "epcoh 10 - train_score 0.4894 - val_score 0.6285 \n",
            "epcoh 11 - train_score 0.4637 - val_score 0.6190 \n",
            "epcoh 12 - train_score 0.4678 - val_score 0.6107 \n",
            "epcoh 13 - train_score 0.4295 - val_score 0.5723 \n",
            "epcoh 14 - train_score 0.4019 - val_score 0.5522 \n",
            "epcoh 15 - train_score 0.3951 - val_score 0.5313 \n",
            "epcoh 16 - train_score 0.3887 - val_score 0.5267 \n",
            "epcoh 17 - train_score 0.3899 - val_score 0.5459 \n",
            "epcoh 18 - train_score 0.3695 - val_score 0.5477 \n",
            "epcoh 19 - train_score 0.3516 - val_score 0.5360 \n",
            "epcoh 20 - train_score 0.3156 - val_score 0.5294 \n",
            "epcoh 21 - train_score 0.3366 - val_score 0.5042 \n",
            "epcoh 22 - train_score 0.3060 - val_score 0.4823 \n",
            "epcoh 23 - train_score 0.2957 - val_score 0.4711 \n",
            "epcoh 24 - train_score 0.2818 - val_score 0.4657 \n",
            "epcoh 25 - train_score 0.2927 - val_score 0.4771 \n",
            "epcoh 26 - train_score 0.2711 - val_score 0.4768 \n",
            "epcoh 27 - train_score 0.2989 - val_score 0.4478 \n",
            "epcoh 28 - train_score 0.2675 - val_score 0.4668 \n",
            "epcoh 29 - train_score 0.2617 - val_score 0.4470 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 0.8501 - val_score 0.8605 \n",
            "epcoh 01 - train_score 0.7195 - val_score 0.8891 \n",
            "epcoh 02 - train_score 0.6777 - val_score 0.8750 \n",
            "epcoh 03 - train_score 0.6433 - val_score 0.8185 \n",
            "epcoh 04 - train_score 0.6069 - val_score 0.7992 \n",
            "epcoh 05 - train_score 0.5803 - val_score 0.7716 \n",
            "epcoh 06 - train_score 0.5606 - val_score 0.7388 \n",
            "epcoh 07 - train_score 0.5399 - val_score 0.7302 \n",
            "epcoh 08 - train_score 0.5103 - val_score 0.7178 \n",
            "epcoh 09 - train_score 0.4917 - val_score 0.7054 \n",
            "epcoh 10 - train_score 0.4568 - val_score 0.6851 \n",
            "epcoh 11 - train_score 0.4519 - val_score 0.6666 \n",
            "epcoh 12 - train_score 0.4325 - val_score 0.6416 \n",
            "epcoh 13 - train_score 0.4172 - val_score 0.6285 \n",
            "epcoh 14 - train_score 0.4174 - val_score 0.6404 \n",
            "epcoh 15 - train_score 0.4121 - val_score 0.6225 \n",
            "epcoh 16 - train_score 0.3693 - val_score 0.6070 \n",
            "epcoh 17 - train_score 0.3626 - val_score 0.5982 \n",
            "epcoh 18 - train_score 0.3501 - val_score 0.5947 \n",
            "epcoh 19 - train_score 0.3265 - val_score 0.6014 \n",
            "epcoh 20 - train_score 0.2966 - val_score 0.5769 \n",
            "epcoh 21 - train_score 0.3103 - val_score 0.5774 \n",
            "epcoh 22 - train_score 0.2847 - val_score 0.5642 \n",
            "epcoh 23 - train_score 0.2998 - val_score 0.5905 \n",
            "epcoh 24 - train_score 0.2776 - val_score 0.5711 \n",
            "epcoh 25 - train_score 0.2487 - val_score 0.5656 \n",
            "epcoh 26 - train_score 0.2688 - val_score 0.5767 \n",
            "epcoh 27 - train_score 0.2598 - val_score 0.5495 \n",
            "epcoh 28 - train_score 0.2438 - val_score 0.5484 \n",
            "epcoh 29 - train_score 0.2472 - val_score 0.5529 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 0.8493 - val_score 0.8603 \n",
            "epcoh 01 - train_score 0.6892 - val_score 0.8697 \n",
            "epcoh 02 - train_score 0.6468 - val_score 0.8570 \n",
            "epcoh 03 - train_score 0.6427 - val_score 0.8243 \n",
            "epcoh 04 - train_score 0.6088 - val_score 0.7906 \n",
            "epcoh 05 - train_score 0.5422 - val_score 0.7799 \n",
            "epcoh 06 - train_score 0.5429 - val_score 0.7337 \n",
            "epcoh 07 - train_score 0.5071 - val_score 0.7135 \n",
            "epcoh 08 - train_score 0.4893 - val_score 0.6882 \n",
            "epcoh 09 - train_score 0.4475 - val_score 0.6705 \n",
            "epcoh 10 - train_score 0.4392 - val_score 0.6588 \n",
            "epcoh 11 - train_score 0.4112 - val_score 0.6276 \n",
            "epcoh 12 - train_score 0.4066 - val_score 0.6249 \n",
            "epcoh 13 - train_score 0.3783 - val_score 0.6238 \n",
            "epcoh 14 - train_score 0.3837 - val_score 0.5971 \n",
            "epcoh 15 - train_score 0.3620 - val_score 0.6007 \n",
            "epcoh 16 - train_score 0.3421 - val_score 0.5862 \n",
            "epcoh 17 - train_score 0.3300 - val_score 0.5738 \n",
            "epcoh 18 - train_score 0.3380 - val_score 0.5585 \n",
            "epcoh 19 - train_score 0.3119 - val_score 0.5949 \n",
            "epcoh 20 - train_score 0.3168 - val_score 0.5593 \n",
            "epcoh 21 - train_score 0.2862 - val_score 0.5468 \n",
            "epcoh 22 - train_score 0.3061 - val_score 0.5561 \n",
            "epcoh 23 - train_score 0.2928 - val_score 0.5518 \n",
            "epcoh 24 - train_score 0.2939 - val_score 0.5370 \n",
            "epcoh 25 - train_score 0.2746 - val_score 0.5374 \n",
            "epcoh 26 - train_score 0.2410 - val_score 0.5293 \n",
            "epcoh 27 - train_score 0.2407 - val_score 0.5341 \n",
            "epcoh 28 - train_score 0.2339 - val_score 0.5238 \n",
            "epcoh 29 - train_score 0.2435 - val_score 0.5127 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 0.8297 - val_score 0.8797 \n",
            "epcoh 01 - train_score 0.6823 - val_score 0.8891 \n",
            "epcoh 02 - train_score 0.6600 - val_score 0.8648 \n",
            "epcoh 03 - train_score 0.6449 - val_score 0.8263 \n",
            "epcoh 04 - train_score 0.6131 - val_score 0.8035 \n",
            "epcoh 05 - train_score 0.5713 - val_score 0.7767 \n",
            "epcoh 06 - train_score 0.5338 - val_score 0.7533 \n",
            "epcoh 07 - train_score 0.5087 - val_score 0.7254 \n",
            "epcoh 08 - train_score 0.4839 - val_score 0.6974 \n",
            "epcoh 09 - train_score 0.4547 - val_score 0.6936 \n",
            "epcoh 10 - train_score 0.4589 - val_score 0.6842 \n",
            "epcoh 11 - train_score 0.4336 - val_score 0.6670 \n",
            "epcoh 12 - train_score 0.4066 - val_score 0.6790 \n",
            "epcoh 13 - train_score 0.3972 - val_score 0.6582 \n",
            "epcoh 14 - train_score 0.3596 - val_score 0.6521 \n",
            "epcoh 15 - train_score 0.3377 - val_score 0.6457 \n",
            "epcoh 16 - train_score 0.3305 - val_score 0.6615 \n",
            "epcoh 17 - train_score 0.3302 - val_score 0.6436 \n",
            "epcoh 18 - train_score 0.3182 - val_score 0.6346 \n",
            "epcoh 19 - train_score 0.2963 - val_score 0.6439 \n",
            "epcoh 20 - train_score 0.3132 - val_score 0.6396 \n",
            "epcoh 21 - train_score 0.2733 - val_score 0.6351 \n",
            "epcoh 22 - train_score 0.2732 - val_score 0.6421 \n",
            "epcoh 23 - train_score 0.2728 - val_score 0.6440 \n",
            "epcoh 24 - train_score 0.2209 - val_score 0.6282 \n",
            "epcoh 25 - train_score 0.2233 - val_score 0.6395 \n",
            "epcoh 26 - train_score 0.2145 - val_score 0.6377 \n",
            "epcoh 27 - train_score 0.2505 - val_score 0.6516 \n",
            "epcoh 28 - train_score 0.2278 - val_score 0.6717 \n",
            "epcoh 29 - train_score 0.1938 - val_score 0.6665 \n",
            "------------------------------------\n",
            "epcoh 00 - train_score 0.8372 - val_score 0.8562 \n",
            "epcoh 01 - train_score 0.6923 - val_score 0.8680 \n",
            "epcoh 02 - train_score 0.6714 - val_score 0.8508 \n",
            "epcoh 03 - train_score 0.6467 - val_score 0.8224 \n",
            "epcoh 04 - train_score 0.6027 - val_score 0.8040 \n",
            "epcoh 05 - train_score 0.5846 - val_score 0.7596 \n",
            "epcoh 06 - train_score 0.5630 - val_score 0.7516 \n",
            "epcoh 07 - train_score 0.5213 - val_score 0.7206 \n",
            "epcoh 08 - train_score 0.5041 - val_score 0.6866 \n",
            "epcoh 09 - train_score 0.4555 - val_score 0.6794 \n",
            "epcoh 10 - train_score 0.4534 - val_score 0.6826 \n",
            "epcoh 11 - train_score 0.4641 - val_score 0.6817 \n",
            "epcoh 12 - train_score 0.4438 - val_score 0.6413 \n",
            "epcoh 13 - train_score 0.3905 - val_score 0.6402 \n",
            "epcoh 14 - train_score 0.3896 - val_score 0.6235 \n",
            "epcoh 15 - train_score 0.3681 - val_score 0.6193 \n",
            "epcoh 16 - train_score 0.3795 - val_score 0.6189 \n",
            "epcoh 17 - train_score 0.3391 - val_score 0.6085 \n",
            "epcoh 18 - train_score 0.3513 - val_score 0.6049 \n",
            "epcoh 19 - train_score 0.3442 - val_score 0.5952 \n",
            "epcoh 20 - train_score 0.3106 - val_score 0.5877 \n",
            "epcoh 21 - train_score 0.2872 - val_score 0.5845 \n",
            "epcoh 22 - train_score 0.3187 - val_score 0.5902 \n",
            "epcoh 23 - train_score 0.2715 - val_score 0.5876 \n",
            "epcoh 24 - train_score 0.2666 - val_score 0.5674 \n",
            "epcoh 25 - train_score 0.2642 - val_score 0.5948 \n",
            "epcoh 26 - train_score 0.2670 - val_score 0.5816 \n",
            "epcoh 27 - train_score 0.2416 - val_score 0.5633 \n",
            "epcoh 28 - train_score 0.2371 - val_score 0.5597 \n",
            "epcoh 29 - train_score 0.2293 - val_score 0.5664 \n",
            "------------------------------------\n",
            "Score: 0.5392 0.0594\n",
            "0.317964537096051\n",
            "Accuracy: 0.9290780141843972\n",
            "F1 score: 0.9290780141843973\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}